{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Faceswap - Google Colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Babaevskiyy/SystemMonitor/blob/main/Faceswap_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihV6led3S0g"
      },
      "source": [
        "# Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLDlBRZ83YBn"
      },
      "source": [
        "Faceswap - Google Colab\n",
        "\n",
        "You also need Faceswap GUI for this to work, since you can't extract faces here, yet.\n",
        "\n",
        "Files you need to upload on Google Drive\n",
        "1.   face_a.zip (source face pics you want to swap)\n",
        "2.   face_b.zip (faces you want to get swapped)\n",
        "\n",
        "> Zipped files with extracted faces and alignments.fsa\n",
        "\n",
        "3.   train.ini\n",
        "> Your train.ini from your local system, in Windows this is in: C:\\Users\\YourName\\faceswap\\config\n",
        "\n",
        "Here is what filesystem looks like in Google Drive\n",
        "*   /colab_files/faceswap/faces/face_a.zip\n",
        "*   /colab_files/faceswap/faces/face_b.zip\n",
        "*   /colab_files/faceswap/config/train.ini\n",
        "\n",
        "Recommended folders to create on Google Drive:\n",
        "*   /colab_files/faceswap/models/YourModelName\n",
        "*   /colab_files/faceswap/output/timelapse\n",
        "\n",
        "Here is what filesystem should look like in this Google Colab after the \"Mount Google Drive\" step below.\n",
        "*   /content/drive/My Drive/colab_files/faceswap/faces/face_a.zip\n",
        "*   /content/drive/My Drive/colab_files/faceswap/faces/face_b.zip\n",
        "*   /content/drive/My Drive/colab_files/faceswap/config/train.ini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Me0Og6vM_N"
      },
      "source": [
        "# Usage Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2aNoS2CvR8m"
      },
      "source": [
        "This is my personal SOP for handling the Faceswap process. I find it is best to tweak your settings, extract, test your training, and convert everything using your local system.\n",
        "\n",
        "But training on my laptop took weeks to get the same results that you can get from just days by using a Google Colab.\n",
        "\n",
        "*   Extract, Align, then Zip all Faces A\n",
        "*   Extract, Align, then Zip all Faces B\n",
        "*   Modify the \"Run Training\" script below to your preferences\n",
        "*   Train in this Colab Notebook\n",
        "*   Watch Google Drive timestamp folder to see how the training is improving\n",
        "*   Download YourModelName to your local system\n",
        "*   Convert using the downloaded model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nw9Lc-sGQvZ"
      },
      "source": [
        "# Keep Session Alive\n",
        " (it may not work due to latest Google Colab Captcha changes.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1w3gNJgtBw5F",
        "outputId": "dff6d755-94fd-4809-f592-c531ecccaf6c"
      },
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\");\n",
        "     btn.click()\n",
        "     }\n",
        "\n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\");\n",
        "     btn.click()\n",
        "     }\n",
        "  }\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              " function ClickConnect(){\n",
              "   btn = document.querySelector(\"colab-connect-button\")\n",
              "   if (btn != null){\n",
              "     console.log(\"Click colab-connect-button\"); \n",
              "     btn.click() \n",
              "     }\n",
              "   \n",
              "   btn = document.getElementById('ok')\n",
              "   if (btn != null){\n",
              "     console.log(\"Click reconnect\"); \n",
              "     btn.click() \n",
              "     }\n",
              "  }\n",
              "  \n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0-VFf7-GYIt"
      },
      "source": [
        "# Check your GPU\n",
        "If it isn't **Tesla T4** or **Tesla P100**, go Runtime->Factory Reset Runtime until you get one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1F6HO7TJKyj",
        "outputId": "fea0f7d2-542a-42ac-c33d-c78fbb175b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version: \" + tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 17 12:20:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Tensorflow version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1yNPAbSSKdK"
      },
      "source": [
        "# Setup Faceswap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0sa6XRJ-33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae2e0c2-e7b7-49a5-d6f0-aa4e9dd03990"
      },
      "source": [
        "#@title Set Time Zone\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Turkey /etc/localtime\n",
        "!date\n",
        "\n",
        "#above is for HST, you can find yours in\n",
        "#/usr/share/zoneinfo\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 17 03:20:36 PM +03 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuNRgHGRSP59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cfce89-4a40-4a5e-f2ef-c1a87508bef5"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGs28NN6VuSD",
        "outputId": "291e8b94-1535-4358-e8f0-e37ee0281229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Get your training data(aka the faces you extracted in app)\n",
        "!cp \"/content/drive/My Drive/colab_files/faceswap/faces/face_a.zip\" .\n",
        "!cp \"/content/drive/My Drive/colab_files/faceswap/faces/face_b.zip\" .\n",
        "\n",
        "!unzip face_a.zip -d face_a\n",
        "!unzip face_b.zip -d face_b\n",
        "\n",
        "!rm face_a.zip\n",
        "!rm face_b.zip\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  face_a.zip\n",
            "  inflating: face_a/025_0.png        \n",
            "  inflating: face_a/006_0.png        \n",
            "  inflating: face_a/007_0.png        \n",
            "  inflating: face_a/008_0.png        \n",
            "  inflating: face_a/009_1.png        \n",
            "  inflating: face_a/010_1.png        \n",
            "  inflating: face_a/011_0.png        \n",
            "  inflating: face_a/012_0.png        \n",
            "  inflating: face_a/013_0.png        \n",
            "  inflating: face_a/014_0.png        \n",
            "  inflating: face_a/015_0.png        \n",
            "  inflating: face_a/016_0.png        \n",
            "  inflating: face_a/017_0.png        \n",
            "  inflating: face_a/018_0.png        \n",
            "  inflating: face_a/019_0.png        \n",
            "  inflating: face_a/020_0.png        \n",
            "  inflating: face_a/021_0.png        \n",
            "  inflating: face_a/022_0.png        \n",
            "  inflating: face_a/023_0.png        \n",
            "  inflating: face_a/024_0.png        \n",
            "Archive:  face_b.zip\n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000403_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000156_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000157_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000158_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000159_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000160_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000161_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000162_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000163_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000164_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000165_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000166_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000167_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000168_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000169_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000170_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000171_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000172_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000173_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000174_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000175_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000176_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000177_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000178_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000179_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000180_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000181_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000182_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000183_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000184_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000185_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000186_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000187_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000188_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000189_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000190_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000191_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000334_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000335_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000336_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000337_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000338_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000339_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000340_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000341_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000342_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000343_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000344_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000345_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000346_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000347_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000350_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000352_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000353_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000354_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000355_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000356_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000357_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000358_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000358_1.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000359_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000360_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000361_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000362_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000363_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000364_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000365_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000366_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000367_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000368_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000369_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000370_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000371_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000372_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000373_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000374_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000375_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000376_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000377_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000378_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000379_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000380_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000381_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000382_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000383_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000384_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000385_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000386_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000387_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000388_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000389_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000390_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000391_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000392_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000393_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000394_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000395_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000396_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000397_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000398_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000399_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000400_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000401_0.png  \n",
            "  inflating: face_b/SnapReels.Net_AQOyVn0bFsuKONa7x2gnBWeRG4nPpeMeSbGtQkvMPX6LxRHvbLc-fVgir9f0YluxqRk7JZ4kyq9_R1l_q-x5RIb9ftq6jQsGO62smoo_000402_0.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUHDDCPCcDKW",
        "outputId": "13e7f12b-a3a7-4d30-d090-199bd84769d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Get the latest Faceswap\n",
        "!git clone https://github.com/deepfakes/faceswap.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'faceswap'...\n",
            "remote: Enumerating objects: 14700, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 14700 (delta 25), reused 20 (delta 20), pack-reused 14661 (from 3)\u001b[K\n",
            "Receiving objects: 100% (14700/14700), 198.72 MiB | 34.51 MiB/s, done.\n",
            "Resolving deltas: 100% (10489/10489), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP9m733KiKG6",
        "outputId": "9f6439ee-5303-4b57-e1fc-9b6c06e6f588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Copy your configuration(train.ini) file\n",
        "!cp \"/content/drive/My Drive/colab_files/faceswap/config/train.ini\" faceswap/config/\n",
        "!ls -lA faceswap/config/\n",
        "!cat faceswap/config/train.ini"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108\n",
            "-rw-r--r-- 1 root root      0 Aug 17 12:50 .keep\n",
            "-rw------- 1 root root 109763 Aug 17 12:50 train.ini\n",
            "[global]\n",
            "# НАСТРОЙКИ, ПРИМЕНИМЫЕ КО ВСЕМ МОДЕЛЯМ\n",
            "# ПРИМЕЧАНИЕ: ДО ТЕХ ПОР, ПОКА ОБ ЭТОМ НЕ СКАЗАНО, ЗНАЧЕНИЯ, ИЗМЕНЕННЫЕ ЗДЕСЬ, БУДУТ ПРИМЕНЕНЫ ПРИ\n",
            "# СОЗДАНИИ НОВОЙ МОДЕЛИ.\n",
            "\n",
            "# Как централизовывать тренировочное изображение. Центр в извлеченных изображениях находится в\n",
            "# середине черепа, основанный на примерной позе лица. Подсекция этих изображений используется для\n",
            "# тренировки. Используемый центр диктует то, как эта подсекция будет обрезана из выравненных\n",
            "# изображений.\n",
            "#     - face: Центрирует учебное изображение по центру лица, регулируя угол наклона и поворота.\n",
            "#     - head: Централизует тренировочное изображение в центре головы, регулируя угол наклона и\n",
            "# \t\tповорота. Примечание: Следует выбирать централизацию головы, если вы планируете включать голову\n",
            "# \t\tполностью (включая волосы) в финальную замену. Может дать смешанные результаты. В дополнении, оно\n",
            "# \t\tстоит того только если вы тренируете с маской, что включает в себя волосы (к примеру: BiSeNet-FP-\n",
            "# \t\tHead).\n",
            "#     - legacy: 'оригинальная' техника извлечения. Централизует тренировочное изображение ближе к\n",
            "# \t\tкончику носа без правок. Может привести к тому, что края лица будут вне тренировочной зоны.\n",
            "# \n",
            "# Выберите из: ['face', 'head', 'legacy']\n",
            "# [По умолчанию: face]\n",
            "centering = face\n",
            "\n",
            "# Сколько извлеченного изображения тренировать. Низкая покрытость ограничит прицел модели к\n",
            "# приближенной центральной зоне, в то время как большие значения могут включать в себя целое лицо.\n",
            "# Существует компромисс между меньшими объемами, дающими больше деталей, и большими объемами,\n",
            "# позволяющими избежать заметных переходов замены. Для централизации 'Face', вам нужно будет оставить\n",
            "# значение выше 75%. Для централизации 'Head', вам скорее всего нужно будет поставить значение 100%.\n",
            "# Адекватные значения для 'Legacy':\n",
            "#     - 62.5% охватывает от бровей до бровей.\n",
            "#     - 75% охватывает от виска до виска.\n",
            "#     - 87.5% охватывает от уха до уха.\n",
            "#     - 100% - полный снимок.\n",
            "# \n",
            "# Выберите десятичное число между 62.5 и 100.0\n",
            "# [По умолчанию: 87.5]\n",
            "coverage = 87.5\n",
            "\n",
            "# Использовать ICNR для чередования инициализатора по умолчанию в повторяющемся шаблоне. Эта стратегия\n",
            "# предназначена для использования в паре с субпиксельным/пиксельным перетасовщиком для уменьшения\n",
            "# \"эффекта шахматной доски\" при реконструкции изображения.\n",
            "#     - [ТОЛЬКО на английском] https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "icnr_init = False\n",
            "\n",
            "# Использовать инициализацию с учетом свертки для сверточных слоев. Это поможет устранить проблему\n",
            "# исчезающего и взрывающегося градиента, а также повысить точность, снизить потери и ускорить\n",
            "# сходимость.\n",
            "# Примечание:\n",
            "#     - При создании новой модели может потребоваться больше видеопамяти, поэтому для первого запуска\n",
            "# \t\tлучше уменьшить размер пачки. Размер пачки может быть увеличен при перезагрузке модели.\n",
            "#     - Использование нескольких видеокарт не поддерживается, поэтому модель следует запускать на\n",
            "# \t\tодной видеокарте. После начала обучения вы можете остановить обучение, включить несколько\n",
            "# \t\tвидеокарт и возобновить его.\n",
            "#     - Построение модели, скорее всего, займет несколько минут, поскольку вычисления для этой техники\n",
            "# \t\tинициализации являются дорогостоящими. Это повлияет только на запуск новой модели.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "conv_aware_init = False\n",
            "\n",
            "# Используемый оптимизатор.\n",
            "#     - adabelief - Адаптация размеров шагов по убеждению в наблюдаемых градиентах('Adapting Stepsizes\n",
            "# \t\tby the Belief in Observed Gradients'). Оптимизатор, цель которого - быстрее сходиться, лучше\n",
            "# \t\tобобщаться и оставаться более стабильным. ([ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/abs/2010.07468). Примечание: значение Epsilon для AdaBelief должно быть меньше,\n",
            "# \t\tчем для других оптимизаторов. Как правило, значение 'Epsilon Exponent' должно быть около '-16'.\n",
            "#     - adam - Адаптивная оптимизация моментов('Adaptive Moment Optimization'). Стохастический метод\n",
            "# \t\tградиентного спуска, основанный на адаптивной оценке моментов первого и второго порядка.\n",
            "#     - nadam - Адаптивная оптимизация моментов с моментумом Нестерова ('Adaptive Moment Optimization\n",
            "# \t\twith Nesterov Momentum'). Похож на Adam, но использует другую формулу для вычисления момента.\n",
            "# rms-prop - Распространение корневого среднего квадрата ('Root Mean Square Propagation').\n",
            "# Поддерживает скользящее (дисконтированное) среднее квадрата градиентов. Делит градиент на корень из\n",
            "# этого среднего.\n",
            "# \n",
            "# Выберите из: ['adabelief', 'adam', 'nadam', 'rms-prop']\n",
            "# [По умолчанию: adam]\n",
            "optimizer = adam\n",
            "\n",
            "# Скорость обучения - насколько быстро ваша модель будет обучаться (насколько огромны изменения весов\n",
            "# модели после одной пачки тренировки). Слишком большие значения могут привести к крахам модели и\n",
            "# невозможности модели найти лучшее решение. Слишком маленькие значения могут привести к невозможности\n",
            "# выбраться из тупиков и найти лучший глобальный минимум.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите десятичное число между 1e-06 и 0.0001\n",
            "# [По умолчанию: 5e-05]\n",
            "learning_rate = 5e-05\n",
            "\n",
            "# Эпсилон добавляет небольшую константу к обновлениям веса, чтобы попытаться избежать ошибок \"деления\n",
            "# на ноль\". Если вы не используете оптимизатор AdaBelief, то, как правило, этот параметр следует\n",
            "# оставить по умолчанию. Для AdaBelief подойдет значение около '-16'.\n",
            "# Во всех случаях, если вы получаете значения потерь 'NaN' и не смогли решить проблему другим способом\n",
            "# (например, увеличив размер пачки или уменьшив скорость обучения), то увеличение эпсилона может\n",
            "# привести к более стабильной модели. Однако это может стоить более медленного обучения и менее\n",
            "# точного конечного результата.\n",
            "# Примечание: Значение, указанное здесь, является \"экспонентой\" к эпсилону. Например, при выборе\n",
            "# значения '-7' эпсилон будет равен 1e-7. При выборе значения \"-3\" эпсилон будет равен 0,001 (1e-3).\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между -20 и 0\n",
            "# [По умолчанию: -7]\n",
            "epsilon_exponent = -7\n",
            "\n",
            "# Когда сохранять веса оптимизатора. Сохранение весов оптимизатора не является необходимым и увеличит\n",
            "# размер файла модели в 3 раза (и соответственно время, необходимое для сохранения модели). Однако\n",
            "# может быть полезно сохранить эти веса, если вы хотите гарантировать, что возобновленная модель\n",
            "# продолжит работу именно с того места, где она остановилась, а не тратит несколько сотен итераций на\n",
            "# догонялки.\n",
            "#     - never - не сохранять веса оптимизатора.\n",
            "#     - always -  сохранять веса оптимизатора при каждой итерации сохранения. Сохранение модели займет\n",
            "# \t\tбольше времени из-за увеличенного размера файла, но в файле модели всегда будет последнее\n",
            "# \t\tсохраненное состояние оптимизатора.\n",
            "#     - exit -  сохранять веса оптимизатора только при явном завершении модели. Это может быть, когда\n",
            "# \t\tмодель активно останавливается или когда выполняются целевые итерации. Примечание. Если сеанс\n",
            "# \t\tобучения завершается по другой причине (например, отключение питания, ошибка нехватки памяти,\n",
            "# \t\tобнаружение NaN), веса оптимизатора НЕ будут сохранены.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: ['never', 'always', 'exit']\n",
            "# [По умолчанию: exit]\n",
            "save_optimizer = exit\n",
            "\n",
            "# Количество итераций для поиска оптимального коэффициента обучения. Большие значения займут больше\n",
            "# времени, но будут более точными.\n",
            "# \n",
            "# Выберите число между 100 и 10000\n",
            "# [По умолчанию: 1000]\n",
            "lr_finder_iterations = 1000\n",
            "\n",
            "# Режим работы для поиска коэффициента обучения. Применимо только для новых моделей. Для уже\n",
            "# существующих моделей режим будет автоматически выставлен в 'set'.\n",
            "#     - set - Обучение с найденным оптимальным коэффициентом обучения.\n",
            "#     - graph_and_set - Вывод графика в папку обучения, показывающего найденные коэффициенты обучения,\n",
            "# \t\tи обучение с оптимальным коэффициентом.\n",
            "#     - graph_and_exit - Вывод графика в папку обучения с найденными коэффициентами обучения с\n",
            "# \t\tпоследующим выходом из программы.\n",
            "# \n",
            "# Выберите из: ['set', 'graph_and_set', 'graph_and_exit']\n",
            "# [По умолчанию: set]\n",
            "lr_finder_mode = set\n",
            "\n",
            "# Насколько агрессивно устанавливать коэффициент обучения. Более агрессивный подход может обучать\n",
            "# быстрее, но с большей вероятностью может привести к взрыву градиентов.\n",
            "#     - default - Оптимальный коэффициент обучения по умолчанию. Безопасный выбор для почти всех\n",
            "# \t\tслучаев использования.\n",
            "#     - aggressive - Устанавливает коэффициент обучения выше, чем по умолчанию. Может обучать быстрее,\n",
            "# \t\tно с большей вероятностью взрыва градиента.\n",
            "#     - extreme - Наивысший оптимальный коэффициент обучения. Гораздо выше риск взрыва градиента.\n",
            "# \n",
            "# Выберите из: ['default', 'aggressive', 'extreme']\n",
            "# [По умолчанию: default]\n",
            "lr_finder_strength = default\n",
            "\n",
            "# Применить AutoClipping к градиентам. AutoClip анализирует веса градиентов и динамически корректирует\n",
            "# значение нормализации, чтобы оно подходило к данным. Может помочь избежать NaN('не число') и\n",
            "# улучшить оптимизацию модели ценой видеопамяти. Ссылка: AutoClip: Adaptive Gradient Clipping for\n",
            "# Source Separation Networks [ТОЛЬКО на английском] https://arxiv.org/abs/2007.14469\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "autoclip = False\n",
            "\n",
            "# Используйте для сверток не нулевую, а отражающую подкладку. Каждая свертка должна заполнять границы\n",
            "# изображения для поддержания правильного размера. Более сложные схемы вставки могут уменьшить\n",
            "# артефакты на границе изображения.\n",
            "#     - http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "reflect_padding = False\n",
            "\n",
            "# [Только для Nvidia]. Включите опцию конфигурации Tensorflow GPU `allow_growth`. Эта опция не\n",
            "# позволяет Tensorflow выделять всю видеопамять видеокарты при запуске, но может привести к повышенной\n",
            "# фрагментации видеопамяти и снижению производительности. Следует включать только в том случае, если у\n",
            "# вас появляются ошибки, рода 'cuDNN fails to initialize'(cuDNN не может инициализироваться) при\n",
            "# начале тренировки.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "allow_growth = False\n",
            "\n",
            "# Видеокарты от NVIDIA могут оперировать в 'float16' быстрее, чем в 'float32'. Смешанная точность\n",
            "# позволяет вам использовать микс float16 с float32, чтобы получить улучшение производительности от\n",
            "# float16 и числовую стабильность от float32.\n",
            "# \n",
            "# Это не было проверено на DirectML, но будет работать на большенстве моделей Nvidia. Оно только\n",
            "# ускорит тренировку на более недавних видеокартах. Те, что имеют возможность вычислений('Compute\n",
            "# Capability') 7.0 и выше, получат самое большое ускорение от смешанной точности, потому что у них\n",
            "# имеются тензор ядра. Старые видеокарты предлагают никакого ускорения от смешанной точности, однако\n",
            "# экономия памяти и (хз, честно, словаря нет) могут дать небольшое ускорение. В основном RTX\n",
            "# видеокарты и позже предлагают самое большое ускорение.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "mixed_precision = False\n",
            "\n",
            "# Если 'Не число'(далее, NaN) сгенерировано в модели - это значит, что модель повреждена и с этого\n",
            "# момента, скорее всего, начнет деградировать. Включение защиты от NaN немедленно остановит\n",
            "# тренировку, в случае, если был обнаружен NaN. Последнее сохранение не будет содержать в себе NaN,\n",
            "# так что у вас будет возможность спасти вашу модель.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "nan_protection = True\n",
            "\n",
            "# [Только для видеокарт] Количество лиц, проходящих через модель в одно время во время конвертирования\n",
            "# \n",
            "# Примечание: Увеличение этого значения вряд ли повлечет за собой ускорение конвертирования, однако,\n",
            "# если у вас появляются ошибки 'Out of Memory', тогда стоит снизить размер пачки.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 1 и 32\n",
            "# [По умолчанию: 16]\n",
            "convert_batchsize = 16\n",
            "\n",
            "[global.loss]\n",
            "# НАСТРОЙКИ ПОТЕРЬ\n",
            "# ПОТЕРЯ - МЕХАНИЗМ, ПО КОТОРОМУ НЕЙРОННАЯ СЕТЬ СУДИТ, НАСКОЛЬКО ХОРОШО ОНА ВОСПРОИЗВОДИТ ЛИЦО.\n",
            "# ПРИМЕЧАНИЕ: ДО ТЕХ ПОР, ПОКА ОБ ЭТОМ НЕ СКАЗАНО, ЗНАЧЕНИЯ, ИЗМЕНЕННЫЕ ЗДЕСЬ, БУДУТ ПРИМЕНЕНЫ ПРИ\n",
            "# СОЗДАНИИ НОВОЙ МОДЕЛИ.\n",
            "\n",
            "# Какую функцию потерь стоит использовать.\n",
            "# \n",
            "#     - ffl: Потеря фокальной частоты. Анализирует частотный спектр изображений, а не сами\n",
            "# \t\tизображения. Эта функция потерь может использоваться сама по себе, но в оригинальной статье было\n",
            "# \t\tобнаружено, что она дает больше преимуществ при использовании в качестве дополнительной потери к\n",
            "# \t\tдругой пространственной функции потерь (например, MSE). Ссылка: Focal Frequency Loss for Image\n",
            "# \t\tReconstruction and Synthesis [ТОЛЬКО на английском] https://arxiv.org/pdf/2012.12821.pdf NB: Эта\n",
            "# \t\tпотеря в настоящее время не работает на картах AMD.\n",
            "# \n",
            "#     - gmsd: Отклонение Схожести Магнитуды Градиентов(Gradient Magnitude Similarity Deviation)\n",
            "# \t\tпытается совместить глобальную стандартную девиацию различий пикселя к пикселю между двумя\n",
            "# \t\tизображениями. Подход похож на SSIM. Ссылка: Gradient Magnitude Similarity Deviation: An Highly\n",
            "# \t\tEfficient Perceptual Image Quality Index [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf\n",
            "# \n",
            "#     - l_inf_norm: Норма L_inf уменьшает наибольшую ошибку отдельного пикселя в изображении. По мере\n",
            "# \t\tпоследовательной минимизации каждой наибольшей ошибки улучшается общая ошибка. Эта потеря будет\n",
            "# \t\tчрезвычайно сосредоточена на выбросах.\n",
            "# \n",
            "#     - laploss: Потеря пирамиды Лапласиана. Пытается улучшить результаты, концентрируясь на краях с\n",
            "# \t\tпомощью пирамид Лапласиана. Поскольку эта функция потерь отдает приоритет краям, а не другой\n",
            "# \t\tнизкочастотной информации, например, цвету, ее не следует использовать самостоятельно. В\n",
            "# \t\tоригинальной реализации эта потеря используется как дополнительная функция к MSE. Ссылка:\n",
            "# \t\tOptimizing the Latent Space of Generative Networks [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/abs/1707.05776\n",
            "# \n",
            "#     - logcosh: log(cosh(x)) действует аналогично MSE для малых ошибок и MAE для больших ошибок. Как\n",
            "# \t\tи MSE, он очень стабилен и предотвращает переборы, когда ошибки близки к нулю. Как и MAE, он\n",
            "# \t\tустойчив к выбросам.\n",
            "# \n",
            "#     - mae: Средняя абсолютная погрешность направляет реконструкцию каждого пикселя к его медианному\n",
            "# \t\tзначению в обучающем наборе данных. Устойчив к выбросам, но в качестве медианы может игнорировать\n",
            "# \t\tнекоторые редкие типы изображений в наборе данных.\n",
            "# \n",
            "#     - ms_ssim: Метрика Индекса Многомасштабного Структурного Сходства (Multiscale Structural\n",
            "# \t\tSimilarity Index Metric) похожа на SSIM, за исключением того, что она выполняет вычисления по\n",
            "# \t\tнескольким масштабам входного изображения.\n",
            "# \n",
            "#     - mse: Средняя квадратичная погрешность направляет реконструкцию каждого пикселя к его среднему\n",
            "# \t\tзначению в наборе данных для обучения. Как среднее значение, оно будет чувствительно к выбросам и\n",
            "# \t\tобычно дает немного более размытые результаты. Ссылка: Multi-Scale Structural Similarity for Image\n",
            "# \t\tQuality Assessment [ТОЛЬКО на английском]https://www.cns.nyu.edu/pub/eero/wang03b.pdf\n",
            "# \n",
            "#     - pixel_gradient_diff: Вместо того чтобы минимизировать разницу между абсолютным значением\n",
            "# \t\tкаждого пикселя в двух образцовых изображениях, вычислить пространственную разницу между пикселями\n",
            "# \t\tв каждом изображении и затем минимизировать эту разницу между двумя изображениями. Это позволяет\n",
            "# \t\tполучить большие цветовые сдвиги, но сохраняет структуру изображения.\n",
            "# \n",
            "#     - smooth_loss: Smooth_L1 - это модификация потери MAE для исправления двух ее недостатков. Эта\n",
            "# \t\tпотеря улучшает стабильность и ориентирование при небольших погрешностях. Ссылка: A General and\n",
            "# \t\tAdaptive Robust Loss Function [ТОЛЬКО на английском] https://arxiv.org/pdf/1701.03077.pdf\n",
            "# \n",
            "#     - ssim: Метрика индекса структурного сходства ('Structural Similarity Index Metric') - это\n",
            "# \t\tоснованная на восприятии потеря, которая учитывает изменения в текстуре, яркости, контрасте и\n",
            "# \t\tлокальной пространственной статистике изображения. Потенциально обеспечивает более реалистичный\n",
            "# \t\tвид изображений. Ссылка: Image Quality Assessment: From Error Visibility to Structural Similarity\n",
            "# \t\t[ТОЛЬКО на английском] http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: ['ffl', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'mae', 'ms_ssim', 'mse',\n",
            "# 'pixel_gradient_diff', 'smooth_loss', 'ssim']\n",
            "# [По умолчанию: ssim]\n",
            "loss_function = ssim\n",
            "\n",
            "# Вторая используемая функция потерь. При использовании потерь, основанных на структуре (таких как\n",
            "# SSIM, MS-SSIM или GMSD), обычно добавляется функция регуляризации L1 (MAE) или регуляризации L2\n",
            "# (MSE). Вы можете настроить вес этой функции потерь с помощью параметра loss_weight_2.\n",
            "# \n",
            "#     - ffl: Потеря фокальной частоты. Анализирует частотный спектр изображений, а не сами\n",
            "# \t\tизображения. Эта функция потерь может использоваться сама по себе, но в оригинальной статье было\n",
            "# \t\tобнаружено, что она дает больше преимуществ при использовании в качестве дополнительной потери к\n",
            "# \t\tдругой пространственной функции потерь (например, MSE). Ссылка: Focal Frequency Loss for Image\n",
            "# \t\tReconstruction and Synthesis [ТОЛЬКО на английском] https://arxiv.org/pdf/2012.12821.pdf NB: Эта\n",
            "# \t\tпотеря в настоящее время не работает на картах AMD.\n",
            "# \n",
            "#     - flip: Nvidia FLIP. Мера потерь восприятия, которая приближает разницу, воспринимаемую\n",
            "# \t\tчеловеком при быстром чередовании (или перелистывании) двух изображений. Используемая сама по\n",
            "# \t\tсебе, эта функция потерь создает на выходе отчетливую сетку. Однако она может быть полезна при\n",
            "# \t\tиспользовании в качестве дополнительной функции потерь. Ссылка: FLIP: A Difference Evaluator for\n",
            "# \t\tAlternating Images [ТОЛЬКО на английском]:\n",
            "# \t\thttps://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf\n",
            "# \n",
            "#     - gmsd: Отклонение Схожести Магнитуды Градиентов(Gradient Magnitude Similarity Deviation)\n",
            "# \t\tпытается совместить глобальную стандартную девиацию различий пикселя к пикселю между двумя\n",
            "# \t\tизображениями. Подход похож на SSIM. Ссылка: Gradient Magnitude Similarity Deviation: An Highly\n",
            "# \t\tEfficient Perceptual Image Quality Index [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf\n",
            "# \n",
            "#     - l_inf_norm: Норма L_inf уменьшает наибольшую ошибку отдельного пикселя в изображении. По мере\n",
            "# \t\tпоследовательной минимизации каждой наибольшей ошибки улучшается общая ошибка. Эта потеря будет\n",
            "# \t\tчрезвычайно сосредоточена на выбросах.\n",
            "# \n",
            "#     - laploss: Потеря пирамиды Лапласиана. Пытается улучшить результаты, концентрируясь на краях с\n",
            "# \t\tпомощью пирамид Лапласиана. Поскольку эта функция потерь отдает приоритет краям, а не другой\n",
            "# \t\tнизкочастотной информации, например, цвету, ее не следует использовать самостоятельно. В\n",
            "# \t\tоригинальной реализации эта потеря используется как дополнительная функция к MSE. Ссылка:\n",
            "# \t\tOptimizing the Latent Space of Generative Networks [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/abs/1707.05776\n",
            "# \n",
            "#     - logcosh: log(cosh(x)) действует аналогично MSE для малых ошибок и MAE для больших ошибок. Как\n",
            "# \t\tи MSE, он очень стабилен и предотвращает переборы, когда ошибки близки к нулю. Как и MAE, он\n",
            "# \t\tустойчив к выбросам.\n",
            "# \n",
            "#     - lpips_alex: LPIPS - это перцептивная потеря, которая использует в качестве метрики потерь\n",
            "# \t\tвыходные характеристики других предварительно обученных моделей. Имейте в виду, что эта функция\n",
            "# \t\tпотерь использует больше VRAM. При самостоятельном использовании эта потеря создает на выходе\n",
            "# \t\tотчетливый муаровый рисунок, однако она может быть полезна как дополнительная функция потерь.\n",
            "# \t\tВывод этой функции является сильным, поэтому, в зависимости от выбранной вами основной функции\n",
            "# \t\tпотерь, вы вряд ли захотите устанавливать вес выше 25%. Ссылка: The Unreasonable Effectiveness of\n",
            "# \t\tDeep Features as a Perceptual Metric [ТОЛЬКО на английском] http://arxiv.org/abs/1801.03924.\n",
            "# Этот вариант использует основу AlexNet. Это довольно легкая и старая модель, которая лучше всего\n",
            "# показала себя в оригинальной реализации.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - lpips_squeeze: То же, что и lpips_alex, но использует основу SqueezeNet. Более облегченная\n",
            "# \t\tверсия AlexNet.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - lpips_vgg16: То же, что и lpips_alex, но использует основу VGG16. Более тяжелая модель.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - mae: Средняя абсолютная погрешность направляет реконструкцию каждого пикселя к его медианному\n",
            "# \t\tзначению в обучающем наборе данных. Устойчив к выбросам, но в качестве медианы может игнорировать\n",
            "# \t\tнекоторые редкие типы изображений в наборе данных.\n",
            "# \n",
            "#     - ms_ssim: Метрика Индекса Многомасштабного Структурного Сходства (Multiscale Structural\n",
            "# \t\tSimilarity Index Metric) похожа на SSIM, за исключением того, что она выполняет вычисления по\n",
            "# \t\tнескольким масштабам входного изображения.\n",
            "# \n",
            "#     - mse: Средняя квадратичная погрешность направляет реконструкцию каждого пикселя к его среднему\n",
            "# \t\tзначению в наборе данных для обучения. Как среднее значение, оно будет чувствительно к выбросам и\n",
            "# \t\tобычно дает немного более размытые результаты. Ссылка: Multi-Scale Structural Similarity for Image\n",
            "# \t\tQuality Assessment [ТОЛЬКО на английском]https://www.cns.nyu.edu/pub/eero/wang03b.pdf\n",
            "# \n",
            "#     - none: Не использовать функцию дополнительных потерь.\n",
            "# \n",
            "#     - pixel_gradient_diff: Вместо того чтобы минимизировать разницу между абсолютным значением\n",
            "# \t\tкаждого пикселя в двух образцовых изображениях, вычислить пространственную разницу между пикселями\n",
            "# \t\tв каждом изображении и затем минимизировать эту разницу между двумя изображениями. Это позволяет\n",
            "# \t\tполучить большие цветовые сдвиги, но сохраняет структуру изображения.\n",
            "# \n",
            "#     - smooth_loss: Smooth_L1 - это модификация потери MAE для исправления двух ее недостатков. Эта\n",
            "# \t\tпотеря улучшает стабильность и ориентирование при небольших погрешностях. Ссылка: A General and\n",
            "# \t\tAdaptive Robust Loss Function [ТОЛЬКО на английском] https://arxiv.org/pdf/1701.03077.pdf\n",
            "# \n",
            "#     - ssim: Метрика индекса структурного сходства ('Structural Similarity Index Metric') - это\n",
            "# \t\tоснованная на восприятии потеря, которая учитывает изменения в текстуре, яркости, контрасте и\n",
            "# \t\tлокальной пространственной статистике изображения. Потенциально обеспечивает более реалистичный\n",
            "# \t\tвид изображений. Ссылка: Image Quality Assessment: From Error Visibility to Structural Similarity\n",
            "# \t\t[ТОЛЬКО на английском] http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: ['ffl', 'flip', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'lpips_alex',\n",
            "# 'lpips_squeeze', 'lpips_vgg16', 'mae', 'ms_ssim', 'mse', 'none', 'pixel_gradient_diff',\n",
            "# 'smooth_loss', 'ssim']\n",
            "# [По умолчанию: mse]\n",
            "loss_function_2 = mse\n",
            "\n",
            "# Величина веса, применяемая ко второй функции потерь.\n",
            "# \n",
            "# \n",
            "# \n",
            "# Значение задается в процентах и показывает, какой вклад выбранная функция должна внести в общую\n",
            "# стоимость потерь модели. Например:\n",
            "#     - 100 - Потери, рассчитанные для четвертой функции потерь, будут применены в полном объеме к\n",
            "# \t\tобщей стоимости потерь.\n",
            "#     - 25 - Потери, рассчитанные для четвертой функции потерь, будут уменьшены на четверть перед\n",
            "# \t\tдобавлением к общей стоимости потерь.\n",
            "#     - 400 - Потери, рассчитанные для четвертой функции потерь, будут умножены в 4 раза перед\n",
            "# \t\tдобавлением к общей оценке потерь.\n",
            "#     - 0 - Полностью отключает четвертую функцию потерь.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 0 и 400\n",
            "# [По умолчанию: 100]\n",
            "loss_weight_2 = 100\n",
            "\n",
            "# Третья используемая функция потерь. Вы можете настроить вес этой функции потерь с помощью параметра\n",
            "# loss_weight_3.\n",
            "# \n",
            "#     - ffl: Потеря фокальной частоты. Анализирует частотный спектр изображений, а не сами\n",
            "# \t\tизображения. Эта функция потерь может использоваться сама по себе, но в оригинальной статье было\n",
            "# \t\tобнаружено, что она дает больше преимуществ при использовании в качестве дополнительной потери к\n",
            "# \t\tдругой пространственной функции потерь (например, MSE). Ссылка: Focal Frequency Loss for Image\n",
            "# \t\tReconstruction and Synthesis [ТОЛЬКО на английском] https://arxiv.org/pdf/2012.12821.pdf NB: Эта\n",
            "# \t\tпотеря в настоящее время не работает на картах AMD.\n",
            "# \n",
            "#     - flip: Nvidia FLIP. Мера потерь восприятия, которая приближает разницу, воспринимаемую\n",
            "# \t\tчеловеком при быстром чередовании (или перелистывании) двух изображений. Используемая сама по\n",
            "# \t\tсебе, эта функция потерь создает на выходе отчетливую сетку. Однако она может быть полезна при\n",
            "# \t\tиспользовании в качестве дополнительной функции потерь. Ссылка: FLIP: A Difference Evaluator for\n",
            "# \t\tAlternating Images [ТОЛЬКО на английском]:\n",
            "# \t\thttps://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf\n",
            "# \n",
            "#     - gmsd: Отклонение Схожести Магнитуды Градиентов(Gradient Magnitude Similarity Deviation)\n",
            "# \t\tпытается совместить глобальную стандартную девиацию различий пикселя к пикселю между двумя\n",
            "# \t\tизображениями. Подход похож на SSIM. Ссылка: Gradient Magnitude Similarity Deviation: An Highly\n",
            "# \t\tEfficient Perceptual Image Quality Index [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf\n",
            "# \n",
            "#     - l_inf_norm: Норма L_inf уменьшает наибольшую ошибку отдельного пикселя в изображении. По мере\n",
            "# \t\tпоследовательной минимизации каждой наибольшей ошибки улучшается общая ошибка. Эта потеря будет\n",
            "# \t\tчрезвычайно сосредоточена на выбросах.\n",
            "# \n",
            "#     - laploss: Потеря пирамиды Лапласиана. Пытается улучшить результаты, концентрируясь на краях с\n",
            "# \t\tпомощью пирамид Лапласиана. Поскольку эта функция потерь отдает приоритет краям, а не другой\n",
            "# \t\tнизкочастотной информации, например, цвету, ее не следует использовать самостоятельно. В\n",
            "# \t\tоригинальной реализации эта потеря используется как дополнительная функция к MSE. Ссылка:\n",
            "# \t\tOptimizing the Latent Space of Generative Networks [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/abs/1707.05776\n",
            "# \n",
            "#     - logcosh: log(cosh(x)) действует аналогично MSE для малых ошибок и MAE для больших ошибок. Как\n",
            "# \t\tи MSE, он очень стабилен и предотвращает переборы, когда ошибки близки к нулю. Как и MAE, он\n",
            "# \t\tустойчив к выбросам.\n",
            "# \n",
            "#     - lpips_alex: LPIPS - это перцептивная потеря, которая использует в качестве метрики потерь\n",
            "# \t\tвыходные характеристики других предварительно обученных моделей. Имейте в виду, что эта функция\n",
            "# \t\tпотерь использует больше VRAM. При самостоятельном использовании эта потеря создает на выходе\n",
            "# \t\tотчетливый муаровый рисунок, однако она может быть полезна как дополнительная функция потерь.\n",
            "# \t\tВывод этой функции является сильным, поэтому, в зависимости от выбранной вами основной функции\n",
            "# \t\tпотерь, вы вряд ли захотите устанавливать вес выше 25%. Ссылка: The Unreasonable Effectiveness of\n",
            "# \t\tDeep Features as a Perceptual Metric [ТОЛЬКО на английском] http://arxiv.org/abs/1801.03924.\n",
            "# Этот вариант использует основу AlexNet. Это довольно легкая и старая модель, которая лучше всего\n",
            "# показала себя в оригинальной реализации.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - lpips_squeeze: То же, что и lpips_alex, но использует основу SqueezeNet. Более облегченная\n",
            "# \t\tверсия AlexNet.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - lpips_vgg16: То же, что и lpips_alex, но использует основу VGG16. Более тяжелая модель.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - mae: Средняя абсолютная погрешность направляет реконструкцию каждого пикселя к его медианному\n",
            "# \t\tзначению в обучающем наборе данных. Устойчив к выбросам, но в качестве медианы может игнорировать\n",
            "# \t\tнекоторые редкие типы изображений в наборе данных.\n",
            "# \n",
            "#     - ms_ssim: Метрика Индекса Многомасштабного Структурного Сходства (Multiscale Structural\n",
            "# \t\tSimilarity Index Metric) похожа на SSIM, за исключением того, что она выполняет вычисления по\n",
            "# \t\tнескольким масштабам входного изображения.\n",
            "# \n",
            "#     - mse: Средняя квадратичная погрешность направляет реконструкцию каждого пикселя к его среднему\n",
            "# \t\tзначению в наборе данных для обучения. Как среднее значение, оно будет чувствительно к выбросам и\n",
            "# \t\tобычно дает немного более размытые результаты. Ссылка: Multi-Scale Structural Similarity for Image\n",
            "# \t\tQuality Assessment [ТОЛЬКО на английском]https://www.cns.nyu.edu/pub/eero/wang03b.pdf\n",
            "# \n",
            "#     - none: Не использовать функцию дополнительных потерь.\n",
            "# \n",
            "#     - pixel_gradient_diff: Вместо того чтобы минимизировать разницу между абсолютным значением\n",
            "# \t\tкаждого пикселя в двух образцовых изображениях, вычислить пространственную разницу между пикселями\n",
            "# \t\tв каждом изображении и затем минимизировать эту разницу между двумя изображениями. Это позволяет\n",
            "# \t\tполучить большие цветовые сдвиги, но сохраняет структуру изображения.\n",
            "# \n",
            "#     - smooth_loss: Smooth_L1 - это модификация потери MAE для исправления двух ее недостатков. Эта\n",
            "# \t\tпотеря улучшает стабильность и ориентирование при небольших погрешностях. Ссылка: A General and\n",
            "# \t\tAdaptive Robust Loss Function [ТОЛЬКО на английском] https://arxiv.org/pdf/1701.03077.pdf\n",
            "# \n",
            "#     - ssim: Метрика индекса структурного сходства ('Structural Similarity Index Metric') - это\n",
            "# \t\tоснованная на восприятии потеря, которая учитывает изменения в текстуре, яркости, контрасте и\n",
            "# \t\tлокальной пространственной статистике изображения. Потенциально обеспечивает более реалистичный\n",
            "# \t\tвид изображений. Ссылка: Image Quality Assessment: From Error Visibility to Structural Similarity\n",
            "# \t\t[ТОЛЬКО на английском] http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: ['ffl', 'flip', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'lpips_alex',\n",
            "# 'lpips_squeeze', 'lpips_vgg16', 'mae', 'ms_ssim', 'mse', 'none', 'pixel_gradient_diff',\n",
            "# 'smooth_loss', 'ssim']\n",
            "# [По умолчанию: none]\n",
            "loss_function_3 = none\n",
            "\n",
            "# Величина веса, применяемая к третьей функции потерь.\n",
            "# \n",
            "# \n",
            "# \n",
            "# Значение задается в процентах и показывает, какой вклад выбранная функция должна внести в общую\n",
            "# стоимость потерь модели. Например:\n",
            "#     - 100 - Потери, рассчитанные для четвертой функции потерь, будут применены в полном объеме к\n",
            "# \t\tобщей стоимости потерь.\n",
            "#     - 25 - Потери, рассчитанные для четвертой функции потерь, будут уменьшены на четверть перед\n",
            "# \t\tдобавлением к общей стоимости потерь.\n",
            "#     - 400 - Потери, рассчитанные для четвертой функции потерь, будут умножены в 4 раза перед\n",
            "# \t\tдобавлением к общей оценке потерь.\n",
            "#     - 0 - Полностью отключает четвертую функцию потерь.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 0 и 400\n",
            "# [По умолчанию: 0]\n",
            "loss_weight_3 = 0\n",
            "\n",
            "# Четвертая используемая функция потерь. Вы можете настроить вес этой функции потерь с помощью\n",
            "# параметра 'loss_weight_4'.\n",
            "# \n",
            "#     - ffl: Потеря фокальной частоты. Анализирует частотный спектр изображений, а не сами\n",
            "# \t\tизображения. Эта функция потерь может использоваться сама по себе, но в оригинальной статье было\n",
            "# \t\tобнаружено, что она дает больше преимуществ при использовании в качестве дополнительной потери к\n",
            "# \t\tдругой пространственной функции потерь (например, MSE). Ссылка: Focal Frequency Loss for Image\n",
            "# \t\tReconstruction and Synthesis [ТОЛЬКО на английском] https://arxiv.org/pdf/2012.12821.pdf NB: Эта\n",
            "# \t\tпотеря в настоящее время не работает на картах AMD.\n",
            "# \n",
            "#     - flip: Nvidia FLIP. Мера потерь восприятия, которая приближает разницу, воспринимаемую\n",
            "# \t\tчеловеком при быстром чередовании (или перелистывании) двух изображений. Используемая сама по\n",
            "# \t\tсебе, эта функция потерь создает на выходе отчетливую сетку. Однако она может быть полезна при\n",
            "# \t\tиспользовании в качестве дополнительной функции потерь. Ссылка: FLIP: A Difference Evaluator for\n",
            "# \t\tAlternating Images [ТОЛЬКО на английском]:\n",
            "# \t\thttps://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf\n",
            "# \n",
            "#     - gmsd: Отклонение Схожести Магнитуды Градиентов(Gradient Magnitude Similarity Deviation)\n",
            "# \t\tпытается совместить глобальную стандартную девиацию различий пикселя к пикселю между двумя\n",
            "# \t\tизображениями. Подход похож на SSIM. Ссылка: Gradient Magnitude Similarity Deviation: An Highly\n",
            "# \t\tEfficient Perceptual Image Quality Index [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf\n",
            "# \n",
            "#     - l_inf_norm: Норма L_inf уменьшает наибольшую ошибку отдельного пикселя в изображении. По мере\n",
            "# \t\tпоследовательной минимизации каждой наибольшей ошибки улучшается общая ошибка. Эта потеря будет\n",
            "# \t\tчрезвычайно сосредоточена на выбросах.\n",
            "# \n",
            "#     - laploss: Потеря пирамиды Лапласиана. Пытается улучшить результаты, концентрируясь на краях с\n",
            "# \t\tпомощью пирамид Лапласиана. Поскольку эта функция потерь отдает приоритет краям, а не другой\n",
            "# \t\tнизкочастотной информации, например, цвету, ее не следует использовать самостоятельно. В\n",
            "# \t\tоригинальной реализации эта потеря используется как дополнительная функция к MSE. Ссылка:\n",
            "# \t\tOptimizing the Latent Space of Generative Networks [ТОЛЬКО на английском]\n",
            "# \t\thttps://arxiv.org/abs/1707.05776\n",
            "# \n",
            "#     - logcosh: log(cosh(x)) действует аналогично MSE для малых ошибок и MAE для больших ошибок. Как\n",
            "# \t\tи MSE, он очень стабилен и предотвращает переборы, когда ошибки близки к нулю. Как и MAE, он\n",
            "# \t\tустойчив к выбросам.\n",
            "# \n",
            "#     - lpips_alex: LPIPS - это перцептивная потеря, которая использует в качестве метрики потерь\n",
            "# \t\tвыходные характеристики других предварительно обученных моделей. Имейте в виду, что эта функция\n",
            "# \t\tпотерь использует больше VRAM. При самостоятельном использовании эта потеря создает на выходе\n",
            "# \t\tотчетливый муаровый рисунок, однако она может быть полезна как дополнительная функция потерь.\n",
            "# \t\tВывод этой функции является сильным, поэтому, в зависимости от выбранной вами основной функции\n",
            "# \t\tпотерь, вы вряд ли захотите устанавливать вес выше 25%. Ссылка: The Unreasonable Effectiveness of\n",
            "# \t\tDeep Features as a Perceptual Metric [ТОЛЬКО на английском] http://arxiv.org/abs/1801.03924.\n",
            "# Этот вариант использует основу AlexNet. Это довольно легкая и старая модель, которая лучше всего\n",
            "# показала себя в оригинальной реализации.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - lpips_squeeze: То же, что и lpips_alex, но использует основу SqueezeNet. Более облегченная\n",
            "# \t\tверсия AlexNet.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - lpips_vgg16: То же, что и lpips_alex, но использует основу VGG16. Более тяжелая модель.\n",
            "# NB: Для пользователей AMD последний линейный слой не реализован.\n",
            "# \n",
            "#     - mae: Средняя абсолютная погрешность направляет реконструкцию каждого пикселя к его медианному\n",
            "# \t\tзначению в обучающем наборе данных. Устойчив к выбросам, но в качестве медианы может игнорировать\n",
            "# \t\tнекоторые редкие типы изображений в наборе данных.\n",
            "# \n",
            "#     - ms_ssim: Метрика Индекса Многомасштабного Структурного Сходства (Multiscale Structural\n",
            "# \t\tSimilarity Index Metric) похожа на SSIM, за исключением того, что она выполняет вычисления по\n",
            "# \t\tнескольким масштабам входного изображения.\n",
            "# \n",
            "#     - mse: Средняя квадратичная погрешность направляет реконструкцию каждого пикселя к его среднему\n",
            "# \t\tзначению в наборе данных для обучения. Как среднее значение, оно будет чувствительно к выбросам и\n",
            "# \t\tобычно дает немного более размытые результаты. Ссылка: Multi-Scale Structural Similarity for Image\n",
            "# \t\tQuality Assessment [ТОЛЬКО на английском]https://www.cns.nyu.edu/pub/eero/wang03b.pdf\n",
            "# \n",
            "#     - none: Не использовать функцию дополнительных потерь.\n",
            "# \n",
            "#     - pixel_gradient_diff: Вместо того чтобы минимизировать разницу между абсолютным значением\n",
            "# \t\tкаждого пикселя в двух образцовых изображениях, вычислить пространственную разницу между пикселями\n",
            "# \t\tв каждом изображении и затем минимизировать эту разницу между двумя изображениями. Это позволяет\n",
            "# \t\tполучить большие цветовые сдвиги, но сохраняет структуру изображения.\n",
            "# \n",
            "#     - smooth_loss: Smooth_L1 - это модификация потери MAE для исправления двух ее недостатков. Эта\n",
            "# \t\tпотеря улучшает стабильность и ориентирование при небольших погрешностях. Ссылка: A General and\n",
            "# \t\tAdaptive Robust Loss Function [ТОЛЬКО на английском] https://arxiv.org/pdf/1701.03077.pdf\n",
            "# \n",
            "#     - ssim: Метрика индекса структурного сходства ('Structural Similarity Index Metric') - это\n",
            "# \t\tоснованная на восприятии потеря, которая учитывает изменения в текстуре, яркости, контрасте и\n",
            "# \t\tлокальной пространственной статистике изображения. Потенциально обеспечивает более реалистичный\n",
            "# \t\tвид изображений. Ссылка: Image Quality Assessment: From Error Visibility to Structural Similarity\n",
            "# \t\t[ТОЛЬКО на английском] http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: ['ffl', 'flip', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'lpips_alex',\n",
            "# 'lpips_squeeze', 'lpips_vgg16', 'mae', 'ms_ssim', 'mse', 'none', 'pixel_gradient_diff',\n",
            "# 'smooth_loss', 'ssim']\n",
            "# [По умолчанию: none]\n",
            "loss_function_4 = none\n",
            "\n",
            "# Величина веса, применяемая к четвертой функции потерь.\n",
            "# \n",
            "# \n",
            "# \n",
            "# Значение задается в процентах и показывает, какой вклад выбранная функция должна внести в общую\n",
            "# стоимость потерь модели. Например:\n",
            "#     - 100 - Потери, рассчитанные для четвертой функции потерь, будут применены в полном объеме к\n",
            "# \t\tобщей стоимости потерь.\n",
            "#     - 25 - Потери, рассчитанные для четвертой функции потерь, будут уменьшены на четверть перед\n",
            "# \t\tдобавлением к общей стоимости потерь.\n",
            "#     - 400 - Потери, рассчитанные для четвертой функции потерь, будут умножены в 4 раза перед\n",
            "# \t\tдобавлением к общей оценке потерь.\n",
            "#     - 0 - Полностью отключает четвертую функцию потерь.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 0 и 400\n",
            "# [По умолчанию: 0]\n",
            "loss_weight_4 = 0\n",
            "\n",
            "# Функция потерь, используемая при обучении маски.\n",
            "#     - MAE - средняя абсолютная погрешность('Mean absolute error') направляет реконструкцию каждого\n",
            "# \t\tпикселя к его срединному значению в обучающем наборе данных. Устойчива к выбросам, но как медиана\n",
            "# \t\tможет игнорировать некоторые редкие типы изображений в наборе данных.\n",
            "#     - MSE - средняя квадратичная погрешность('Mean squared error') направляет реконструкцию каждого\n",
            "# \t\tпикселя к его срединному значению в обучающем наборе данных. Как среднее значение, оно\n",
            "# \t\tчувствительно к выбросам и обычно дает немного более размытые результаты.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите из: ['mae', 'mse']\n",
            "# [По умолчанию: mse]\n",
            "mask_loss_function = mse\n",
            "\n",
            "# Величина приоритета, которую следует придать глазам.\n",
            "# \n",
            "# Значение дается как множитель основного показателя потерь. Например:\n",
            "#     - 1 - Глаза получат тот же приоритет, что и остальное лицо.\n",
            "#     - 10 - глаза получат оценку в 10 раз выше, чем остальные части лица.\n",
            "# \n",
            "# NB: Penalized Mask Loss должен быть включен, чтобы использовать эту опцию.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 1 и 40\n",
            "# [По умолчанию: 3]\n",
            "eye_multiplier = 3\n",
            "\n",
            "# Величина приоритета, которую следует придать рту.\n",
            "# \n",
            "# Значение дается как множитель основного показателя потерь. Например:\n",
            "#     - 1 - Рот получит тот же приоритет, что и остальное лицо.\n",
            "#     - 10 - Рот получит оценку в 10 раз выше, чем остальные части лица.\n",
            "# \n",
            "# NB: Penalized Mask Loss должен быть включен, чтобы использовать эту опцию.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 1 и 40\n",
            "# [По умолчанию: 2]\n",
            "mouth_multiplier = 2\n",
            "\n",
            "# Функция потерь изображения взвешивается по наличию маски. Для областей изображения без маски лица\n",
            "# погрешности реконструкции игнорируются, в то время как область лица с маской является приоритетной.\n",
            "# Может повысить общее качество за счет концентрации внимания на основной области лица.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "penalized_mask_loss = True\n",
            "\n",
            "# Маска, которая будет использоваться для обучения. Если вы выбрали 'Learn Mask' или 'Penalized Mask\n",
            "# Loss', вы должны выбрать значение, отличное от 'none'. Необходимая маска должна быть выбрана в\n",
            "# процессе извлечения. Если она не существует в файле выравниваний, то она будет создана до начала\n",
            "# обучения.\n",
            "#     - none: Не использовать маску.\n",
            "#     - bisenet-fp_face: Относительно легкая маска на основе NN, которая обеспечивает более точный\n",
            "# \t\tконтроль над маскируемой областью (настраивается в настройках маски). Используйте эту версию\n",
            "# \t\tbisenet-fp, если ваша модель обучена с центрированием 'face' или 'legacy'.\n",
            "#     - bisenet-fp_head: Относительно легкая маска на основе NN, которая обеспечивает более точный\n",
            "# \t\tконтроль над маскируемой областью (настраивается в параметрах маски). Используйте эту версию\n",
            "# \t\tbisenet-fp, если ваша модель обучена с центрированием 'head'.\n",
            "#     - components: Маска, разработанная для сегментации лица на основе расположения ориентиров. Для\n",
            "# \t\tсоздания маски вокруг внешних ориентиров строится выпуклая оболочка.\n",
            "#     - custom_face: Пользовательская маска, созданная пользователем и центрированная по лицу.\n",
            "#     - custom_head: Созданная пользователем маска, центрированная по голове.\n",
            "#     - extended: Маска, разработанная для сегментации лица на основе расположения ориентиров.\n",
            "# \t\tВыпуклый корпус строится вокруг внешних ориентиров, и маска расширяется вверх на лоб.\n",
            "#     - vgg-clear: Маска предназначена для интеллектуальной сегментации преимущественно фронтальных\n",
            "# \t\tлиц без препятствий. Профильные лица и препятствия могут привести к снижению производительности.\n",
            "#     - vgg-obstructed: Маска, разработанная для интеллектуальной сегментации преимущественно\n",
            "# \t\tфронтальных лиц. Модель маски была специально обучена распознавать некоторые препятствия на лице\n",
            "# \t\t(руки и очки). Профильные лица могут иметь низкую производительность.\n",
            "#     - unet-dfl: Маска, разработанная для интеллектуальной сегментации преимущественно фронтальных\n",
            "# \t\tлиц. Модель маски была обучена членами сообщества и для дальнейшего описания нуждается в\n",
            "# \t\tтестировании. Профильные лица могут иметь низкую производительность.\n",
            "# \n",
            "# Выберите из: ['none', 'bisenet-fp_face', 'bisenet-fp_head', 'components', 'custom_face',\n",
            "# 'custom_head', 'extended', 'unet-dfl', 'vgg-clear', 'vgg-obstructed']\n",
            "# [По умолчанию: extended]\n",
            "mask_type = extended\n",
            "\n",
            "# Расширяет или сужает маску. Отрицательные значения сужают маску (делают её меньше). Положительные\n",
            "# значения расширяют маску (делают её больше).\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите десятичное число между -5.0 и 5.0\n",
            "# [По умолчанию: 0]\n",
            "mask_dilation = 0\n",
            "\n",
            "# Применить размытие по Гауссу на входную маску. Дает эффект сглаживания краев маски, что может помочь\n",
            "# с плохо вычисленными масками и дает менее резкий край предугаданной маске. Размер в пикселях\n",
            "# (вычисленно из маски на 128 пикселей). Установите 0, чтобы не применять размытие по Гауссу. Это\n",
            "# значение должно быть нечетным, если передано четное число, то оно будет округлено до следующего\n",
            "# нечетного числа.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 0 и 9\n",
            "# [По умолчанию: 3]\n",
            "mask_blur_kernel = 3\n",
            "\n",
            "# Устанавливает пиксели, которые почти белые - в белые и которые почти черные - в черные. Установите\n",
            "# 0, чтобы выключить.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 0 и 50\n",
            "# [По умолчанию: 4]\n",
            "mask_threshold = 4\n",
            "\n",
            "# Выделить частичку модели обучению тому, как дублировать входную маску. Увеличивает использование\n",
            "# видеопамяти в обмен на обучение быстрой способности попытки переделывать более сложные маски.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "learn_mask = False\n",
            "\n",
            "[model.dfaker]\n",
            "# DFAKER MODEL (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)\n",
            "\n",
            "# Resolution (in pixels) of the output image to generate on.\n",
            "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
            "# Must be 128 or 256.\n",
            "# \n",
            "# Выберите число между 128 и 256\n",
            "# [По умолчанию: 128]\n",
            "output_size = 128\n",
            "\n",
            "[model.dfl_h128]\n",
            "# DFL H128 MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)\n",
            "\n",
            "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
            "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "lowmem = False\n",
            "\n",
            "[model.dfl_sae]\n",
            "# DFL SAE MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)\n",
            "\n",
            "# Resolution (in pixels) of the input image to train on.\n",
            "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
            "# \n",
            "# Must be divisible by 16.\n",
            "# \n",
            "# Выберите число между 64 и 256\n",
            "# [По умолчанию: 128]\n",
            "input_size = 128\n",
            "\n",
            "# Model architecture:\n",
            "#     - 'df': Keeps the faces more natural.\n",
            "#     - 'liae': Can help fix overly different face shapes.\n",
            "# \n",
            "# Выберите из: ['df', 'liae']\n",
            "# [По умолчанию: df]\n",
            "architecture = df\n",
            "\n",
            "# Face information is stored in AutoEncoder dimensions. If there are not enough dimensions then\n",
            "# certain facial features may not be recognized.\n",
            "# Higher number of dimensions are better, but require more VRAM.\n",
            "# Set to 0 to use the architecture defaults (256 for liae, 512 for df).\n",
            "# \n",
            "# Выберите число между 0 и 1024\n",
            "# [По умолчанию: 0]\n",
            "autoencoder_dims = 0\n",
            "\n",
            "# Encoder dimensions per channel. Higher number of encoder dimensions will help the model to recognize\n",
            "# more facial features, but will require more VRAM.\n",
            "# \n",
            "# Выберите число между 21 и 85\n",
            "# [По умолчанию: 42]\n",
            "encoder_dims = 42\n",
            "\n",
            "# Decoder dimensions per channel. Higher number of decoder dimensions will help the model to improve\n",
            "# details, but will require more VRAM.\n",
            "# \n",
            "# Выберите число между 10 и 85\n",
            "# [По умолчанию: 21]\n",
            "decoder_dims = 21\n",
            "\n",
            "# Multiscale decoder can help to obtain better details.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "multiscale_decoder = False\n",
            "\n",
            "[model.dlight]\n",
            "# A LIGHTWEIGHT, HIGH RESOLUTION DFAKER VARIANT (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)\n",
            "\n",
            "# Higher settings will allow learning more features such as tatoos, piercing and wrinkles.\n",
            "# Strongly affects VRAM usage.\n",
            "# \n",
            "# Выберите из: ['lowmem', 'fair', 'best']\n",
            "# [По умолчанию: best]\n",
            "features = best\n",
            "\n",
            "# Defines detail fidelity. Lower setting can appear 'rugged' while 'good' might take a longer time to\n",
            "# train.\n",
            "# Affects VRAM usage.\n",
            "# \n",
            "# Выберите из: ['fast', 'good']\n",
            "# [По умолчанию: good]\n",
            "details = good\n",
            "\n",
            "# Output image resolution (in pixels).\n",
            "# Be aware that larger resolution will increase VRAM requirements.\n",
            "# NB: Must be either 128, 256, or 384.\n",
            "# \n",
            "# Выберите число между 128 и 384\n",
            "# [По умолчанию: 256]\n",
            "output_size = 256\n",
            "\n",
            "[model.original]\n",
            "# ORIGINAL FACESWAP MODEL.\n",
            "\n",
            "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
            "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "lowmem = False\n",
            "\n",
            "[model.phaze_a]\n",
            "# PHAZE-A MODEL BY TORZDF, WITH THANKS TO BIRBFAKES.\n",
            "# ALLOWS FOR THE EXPERIMENTATION OF VARIOUS STANDARD NETWORKS AS THE ENCODER AND TAKES INSPIRATION\n",
            "# FROM NVIDIA'S STYLEGAN FOR THE DECODER. IT IS HIGHLY RECOMMENDED TO RESEARCH TO UNDERSTAND THE\n",
            "# PARAMETERS BETTER.\n",
            "\n",
            "# Resolution (in pixels) of the output image to generate.\n",
            "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
            "# \n",
            "# Выберите число между 64 и 2048\n",
            "# [По умолчанию: 128]\n",
            "output_size = 128\n",
            "\n",
            "# Whether to create a shared fully connected layer. This layer will have the same structure as the\n",
            "# fully connected layers used for each side of the model. A shared fully connected layer looks for\n",
            "# patterns that are common to both sides. NB: Enabling this option only makes sense if 'split fc' is\n",
            "# selected.\n",
            "#     - none - Do not create a Fully Connected layer for shared data. (Original method)\n",
            "#     - full - Create an exclusive Fully Connected layer for shared data. (IAE method)\n",
            "#     - half - Use the 'fc_a' layer for shared data. This saves VRAM by re-using the 'A' side's fully\n",
            "# \t\tconnected model for the shared data. However, this will lead to an 'unbalanced' model and can lead\n",
            "# \t\tto more identity bleed (DFL method)\n",
            "# \n",
            "# Выберите из: ['none', 'full', 'half']\n",
            "# [По умолчанию: none]\n",
            "shared_fc = none\n",
            "\n",
            "# Whether to enable the G-Block. If enabled, this will create a shared fully connected layer\n",
            "# (configurable in the 'G-Block hidden layers' section) to look for patterns in the combined data,\n",
            "# before feeding a block prior to the decoder for merging this shared and combined data.\n",
            "#     - True - Use the G-Block in the Decoder. A combined fully connected layer will be created to\n",
            "# \t\tfeed this block which can be configured below.\n",
            "#     - False - Don't use the G-Block in the decoder. No combined fully connected layer will be\n",
            "# \t\tcreated.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "enable_gblock = True\n",
            "\n",
            "# Whether to use a single shared Fully Connected layer or separate Fully Connected layers for each\n",
            "# side.\n",
            "#     - True - Use separate Fully Connected layers for Face A and Face B. This is more similar to the\n",
            "# \t\t'IAE' style of model.\n",
            "#     - False - Use combined Fully Connected layers for both sides. This is more similar to the\n",
            "# \t\toriginal Faceswap architecture.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "split_fc = True\n",
            "\n",
            "# If the G-Block is enabled, Whether to use a single G-Block shared between both sides, or whether to\n",
            "# have a separate G-Block (one for each side). NB: The Fully Connected layer that feeds the G-Block\n",
            "# will always be shared.\n",
            "#     - True - Use separate G-Blocks for Face A and Face B.\n",
            "#     - False - Use a combined G-Block layers for both sides.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "split_gblock = False\n",
            "\n",
            "# Whether to use a single decoder or split decoders.\n",
            "#     - True - Use a separate decoder for Face A and Face B. This is more similar to the original\n",
            "# \t\tFaceswap architecture.\n",
            "#     - False - Use a combined Decoder. This is more similar to 'IAE' style architecture.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "split_decoders = False\n",
            "\n",
            "# The encoder architecture to use. See the relevant config sections for specific architecture\n",
            "# tweaking.\n",
            "# NB: For keras based pre-built models, the global initializers and padding options will be ignored\n",
            "# for the selected encoder.\n",
            "# \n",
            "#     - CLIPv: This is an implementation of the Visual encoder from the CLIP transformer. The ViT\n",
            "# \t\tweights are trained on imagenet whilst the FaRL weights are trained on face related tasks. All\n",
            "# \t\thave a default input size of 224px except for ViT-L-14-336px that has an input size of 336px. Ref:\n",
            "# \t\tLearning Transferable Visual Models From Natural Language Supervision (2021):\n",
            "# \t\thttps://arxiv.org/abs/2103.00020\n",
            "# \n",
            "#     - densenet: (32px -224px). Ref: Densely Connected Convolutional Networks (2016):\n",
            "# \t\thttps://arxiv.org/abs/1608.06993?source=post_page\n",
            "# \n",
            "#     - efficientnet: [Tensorflow 2.3+ only] EfficientNet has numerous variants (B0 - B8) that\n",
            "# \t\tincreases the model width, depth and dimensional space at each step. The minimum input resolution\n",
            "# \t\tis 32px for all variants. The maximum input resolution for each variant is: b0: 224px, b1: 240px,\n",
            "# \t\tb2: 260px, b3: 300px, b4: 380px, b5: 456px, b6: 528px, b7 600px. Ref: Rethinking Model Scaling for\n",
            "# \t\tConvolutional Neural Networks (2020): https://arxiv.org/abs/1905.11946\n",
            "# \n",
            "#     - efficientnet_v2: [Tensorflow 2.8+ only] EfficientNetV2 is the follow up to efficientnet. It\n",
            "# \t\thas numerous variants (B0 - B3 and Small, Medium and Large) that increases the model width, depth\n",
            "# \t\tand dimensional space at each step. The minimum input resolution is 32px for all variants. The\n",
            "# \t\tmaximum input resolution for each variant is: b0: 224px, b1: 240px, b2: 260px, b3: 300px, s:\n",
            "# \t\t384px, m: 480px, l: 480px. Ref: EfficientNetV2: Smaller Models and Faster Training (2021):\n",
            "# \t\thttps://arxiv.org/abs/2104.00298\n",
            "# \n",
            "#     - fs_original: (32px - 1024px). A configurable variant of the original facewap encoder. ImageNet\n",
            "# \t\tweights cannot be loaded for this model. Additional parameters can be configured with the 'fs_enc'\n",
            "# \t\toptions. A version of this encoder is used in the following models: Original, Original (lowmem),\n",
            "# \t\tDfaker, DFL-H128, DFL-SAE, IAE, Lightweight.\n",
            "# \n",
            "#     - inception_resnet_v2: (75px - 299px). Ref: Inception-ResNet and the Impact of Residual\n",
            "# \t\tConnections on Learning (2016): https://arxiv.org/abs/1602.07261\n",
            "# \n",
            "#     - inceptionV3: (75px - 299px). Ref: Rethinking the Inception Architecture for Computer Vision\n",
            "# \t\t(2015): https://arxiv.org/abs/1512.00567\n",
            "# \n",
            "#     - mobilenet: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'\n",
            "# \t\toptions. Ref: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n",
            "# \t\t(2017): https://arxiv.org/abs/1704.04861\n",
            "# \n",
            "#     - mobilenet_v2: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'\n",
            "# \t\toptions. Ref: MobileNetV2: Inverted Residuals and Linear Bottlenecks (2018):\n",
            "# \t\thttps://arxiv.org/abs/1801.04381\n",
            "# \n",
            "#     - mobilenet_v3: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'\n",
            "# \t\toptions. Ref: Searching for MobileNetV3 (2019): https://arxiv.org/pdf/1905.02244.pdf\n",
            "# \n",
            "#     - nasnet: (32px - 331px (large) or 224px (mobile)). Ref: Learning Transferable Architectures for\n",
            "# \t\tScalable Image Recognition (2017): https://arxiv.org/abs/1707.07012\n",
            "# \n",
            "#     - resnet: (32px - 224px). Deep Residual Learning for Image Recognition (2015):\n",
            "# \t\thttps://arxiv.org/abs/1512.03385\n",
            "# \n",
            "#     - vgg: (32px - 224px). Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
            "# \t\t(2014): https://arxiv.org/abs/1409.1556\n",
            "# \n",
            "#     - xception: (71px - 229px). Ref: Deep Learning with Depthwise Separable Convolutions (2017):\n",
            "# \t\thttps://arxiv.org/abs/1409.1556.\n",
            "# \n",
            "# \n",
            "# Выберите из: ['clipv_farl-b-16-16', 'clipv_farl-b-16-64', 'clipv_vit-b-16', 'clipv_vit-b-32',\n",
            "# 'clipv_vit-l-14', 'clipv_vit-l-14-336px', 'densenet121', 'densenet169', 'densenet201',\n",
            "# 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4',\n",
            "# 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_b0', 'efficientnet_v2_b1',\n",
            "# 'efficientnet_v2_b2', 'efficientnet_v2_b3', 'efficientnet_v2_l', 'efficientnet_v2_m',\n",
            "# 'efficientnet_v2_s', 'fs_original', 'inception_resnet_v2', 'inception_v3', 'mobilenet',\n",
            "# 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'nasnet_large', 'nasnet_mobile',\n",
            "# 'resnet101', 'resnet101_v2', 'resnet152', 'resnet152_v2', 'resnet50', 'resnet50_v2', 'vgg16',\n",
            "# 'vgg19', 'xception']\n",
            "# [По умолчанию: fs_original]\n",
            "enc_architecture = fs_original\n",
            "\n",
            "# Input scaling for the encoder. Some of the encoders have large input sizes, which often are not\n",
            "# helpful for Faceswap. This setting scales the dimensional space that the encoder works in. For\n",
            "# example an encoder with a maximum input size of 224px will be input an image of 112px at 50%%\n",
            "# scaling. See the Architecture tooltip for the minimum and maximum sizes for each encoder. NB: The\n",
            "# input size will be rounded down to the nearest 16 pixels.\n",
            "# \n",
            "# Выберите число между 0 и 200\n",
            "# [По умолчанию: 7]\n",
            "enc_scaling = 7\n",
            "\n",
            "# Load pre-trained weights trained on ImageNet data. Only available for non-Faceswap encoders (i.e.\n",
            "# those not beginning with 'fs'). NB: If you use the global 'load weights' option and have selected to\n",
            "# load weights from a previous model's 'encoder' or 'keras_encoder' then the weights loaded here will\n",
            "# be replaced by the weights loaded from your saved model.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "enc_load_weights = True\n",
            "\n",
            "# The type of layer to use for the bottleneck.\n",
            "#     - average_pooling: Use a Global Average Pooling 2D layer for the bottleneck.\n",
            "#     - dense: Use a Dense layer for the bottleneck (the traditional Faceswap method). You can set the\n",
            "# \t\tsize of the Dense layer with the 'bottleneck_size' parameter.\n",
            "#     - max_pooling: Use a Global Max Pooling 2D layer for the bottleneck.\n",
            "#  latten: Don't use a bottleneck at all. Some encoders output in a size that make a bottleneck\n",
            "# unnecessary. This option flattens the output from the encoder, with no further operations\n",
            "# \n",
            "# Выберите из: ['average_pooling', 'dense', 'max_pooling', 'flatten']\n",
            "# [По умолчанию: dense]\n",
            "bottleneck_type = dense\n",
            "\n",
            "# Apply a normalization layer after encoder output and prior to the bottleneck.\n",
            "#     - none - Do not apply a normalization layer\n",
            "#     - instance - Apply Instance Normalization\n",
            "#     - layer - Apply Layer Normalization (Ba et al., 2016)\n",
            "#     - rms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A simplified version\n",
            "# \t\tof Layer Normalization with reduced overhead.\n",
            "# \n",
            "# Выберите из: ['none', 'instance', 'layer', 'rms']\n",
            "# [По умолчанию: none]\n",
            "bottleneck_norm = none\n",
            "\n",
            "# If using a Dense layer for the bottleneck, then this is the number of nodes to use.\n",
            "# \n",
            "# Выберите число между 128 и 4096\n",
            "# [По умолчанию: 1024]\n",
            "bottleneck_size = 1024\n",
            "\n",
            "# Whether to place the bottleneck in the Encoder or to place it with the other hidden layers. Placing\n",
            "# the bottleneck in the encoder means that both sides will share the same bottleneck. Placing it with\n",
            "# the other fully connected layers means that each fully connected layer will each get their own\n",
            "# bottleneck. This may be combined or split depending on your overall architecture configuration\n",
            "# settings.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "bottleneck_in_encoder = True\n",
            "\n",
            "# The number of consecutive Dense (fully connected) layers to include in each side's intermediate\n",
            "# layer.\n",
            "# \n",
            "# Выберите число между 0 и 16\n",
            "# [По умолчанию: 1]\n",
            "fc_depth = 1\n",
            "\n",
            "# The number of filters to use for the initial fully connected layer. The number of nodes actually\n",
            "# used is: fc_min_filters x fc_dimensions x fc_dimensions.\n",
            "# NB: This value may be scaled down, depending on output resolution.\n",
            "# \n",
            "# Выберите число между 16 и 5120\n",
            "# [По умолчанию: 1024]\n",
            "fc_min_filters = 1024\n",
            "\n",
            "# This is the number of filters to be used in the final reshape layer at the end of the fully\n",
            "# connected layers. The actual number of nodes used for the final fully connected layer is:\n",
            "# fc_min_filters x fc_dimensions x fc_dimensions.\n",
            "# NB: This value may be scaled down, depending on output resolution.\n",
            "# \n",
            "# Выберите число между 128 и 5120\n",
            "# [По умолчанию: 1024]\n",
            "fc_max_filters = 1024\n",
            "\n",
            "# The height and width dimension for the final reshape layer at the end of the fully connected layers.\n",
            "# NB: The total number of nodes within the final fully connected layer will be: fc_dimensions x\n",
            "# fc_dimensions x fc_max_filters.\n",
            "# \n",
            "# Выберите число между 1 и 16\n",
            "# [По умолчанию: 4]\n",
            "fc_dimensions = 4\n",
            "\n",
            "# The rate that the filters move from the minimum number of filters to the maximum number of filters.\n",
            "# EG:\n",
            "# Negative numbers will change the number of filters quicker at first and slow down each layer.\n",
            "# Positive numbers will change the number of filters slower at first but then speed up each layer.\n",
            "# 0.0 - This will change at a linear rate (i.e. the same number of filters will be changed at each\n",
            "# layer).\n",
            "# \n",
            "# Выберите десятичное число между -0.99 и 0.99\n",
            "# [По умолчанию: -0.5]\n",
            "fc_filter_slope = -0.5\n",
            "\n",
            "# Dropout is a form of regularization that can prevent a model from over-fitting and help to keep\n",
            "# neurons 'alive'. 0.5 will dropout half the connections between each fully connected layer, 0.25 will\n",
            "# dropout a quarter of the connections etc. Set to 0.0 to disable.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите десятичное число между 0.0 и 0.99\n",
            "# [По умолчанию: 0.0]\n",
            "fc_dropout = 0.0\n",
            "\n",
            "# The type of dimensional upsampling to perform at the end of the fully connected layers, if upsamples\n",
            "# > 0. The number of filters used for the upscale layers will be the value given in\n",
            "# 'fc_upsample_filters'.\n",
            "#     - upsample2d - A lightweight and VRAM friendly method. 'quick and dirty' but does not learn any\n",
            "# \t\tparameters\n",
            "#     - subpixel - Sub-pixel upscaler using depth-to-space which may require more VRAM.\n",
            "#     - resize_images - Uses the Keras resize_image function to save about half as much vram as the\n",
            "# \t\theaviest methods.\n",
            "#     - upscale_fast - Developed by Andenixa. Focusses on speed to upscale, but requires more VRAM.\n",
            "#     - upscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and Upsampling2D\n",
            "# \t\tto upscale, saving about 1/3rd of VRAM of the heaviest methods.\n",
            "# \n",
            "# Выберите из: ['resize_images', 'subpixel', 'upscale_fast', 'upscale_hybrid', 'upsample2d']\n",
            "# [По умолчанию: upsample2d]\n",
            "fc_upsampler = upsample2d\n",
            "\n",
            "# Some upsampling can occur within the Fully Connected layers rather than in the Decoder to increase\n",
            "# the dimensional space. Set how many upscale layers should occur within the Fully Connected layers.\n",
            "# \n",
            "# Выберите число между 0 и 4\n",
            "# [По умолчанию: 1]\n",
            "fc_upsamples = 1\n",
            "\n",
            "# If you have selected an upsampler which requires filters (i.e. any upsampler with the exception of\n",
            "# Upsampling2D), then this is the number of filters to be used for the upsamplers within the fully\n",
            "# connected layers,  NB: This value may be scaled down, depending on output resolution. Also note,\n",
            "# that this figure will dictate the number of filters used for the G-Block, if selected.\n",
            "# \n",
            "# Выберите число между 128 и 5120\n",
            "# [По умолчанию: 512]\n",
            "fc_upsample_filters = 512\n",
            "\n",
            "# The number of consecutive Dense (fully connected) layers to include in the G-Block shared layer.\n",
            "# \n",
            "# Выберите число между 1 и 16\n",
            "# [По умолчанию: 3]\n",
            "fc_gblock_depth = 3\n",
            "\n",
            "# The number of nodes to use for the initial G-Block shared fully connected layer.\n",
            "# \n",
            "# Выберите число между 128 и 5120\n",
            "# [По умолчанию: 512]\n",
            "fc_gblock_min_nodes = 512\n",
            "\n",
            "# The number of nodes to use for the final G-Block shared fully connected layer.\n",
            "# \n",
            "# Выберите число между 128 и 5120\n",
            "# [По умолчанию: 512]\n",
            "fc_gblock_max_nodes = 512\n",
            "\n",
            "# The rate that the filters move from the minimum number of filters to the maximum number of filters\n",
            "# for the G-Block shared layers. EG:\n",
            "# Negative numbers will change the number of filters quicker at first and slow down each layer.\n",
            "# Positive numbers will change the number of filters slower at first but then speed up each layer.\n",
            "# 0.0 - This will change at a linear rate (i.e. the same number of filters will be changed at each\n",
            "# layer).\n",
            "# \n",
            "# Выберите десятичное число между -0.99 и 0.99\n",
            "# [По умолчанию: -0.5]\n",
            "fc_gblock_filter_slope = -0.5\n",
            "\n",
            "# Dropout is a regularization technique that can prevent a model from over-fitting and help to keep\n",
            "# neurons 'alive'. 0.5 will dropout half the connections between each fully connected layer, 0.25 will\n",
            "# dropout a quarter of the connections etc. Set to 0.0 to disable.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите десятичное число между 0.0 и 0.99\n",
            "# [По умолчанию: 0.0]\n",
            "fc_gblock_dropout = 0.0\n",
            "\n",
            "# The method to use for the upscales within the decoder. Images are upscaled multiple times within the\n",
            "# decoder as the network learns to reconstruct the face.\n",
            "#     - subpixel - Sub-pixel upscaler using depth-to-space which requires more VRAM.\n",
            "#     - resize_images - Uses the Keras resize_image function to save about half as much vram as the\n",
            "# \t\theaviest methods.\n",
            "#     - upscale_fast - Developed by Andenixa. Focusses on speed to upscale, but requires more VRAM.\n",
            "#     - upscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and Upsampling2D\n",
            "# \t\tto upscale, saving about 1/3rd of VRAM of the heaviest methods.\n",
            "#     - upscale_dny - An alternative upscale implementation using Upsampling2D to upsale.\n",
            "# \n",
            "# Выберите из: ['subpixel', 'resize_images', 'upscale_fast', 'upscale_hybrid', 'upscale_dny']\n",
            "# [По умолчанию: subpixel]\n",
            "dec_upscale_method = subpixel\n",
            "\n",
            "# It is possible to place some of the upscales at the end of the fully connected model. For models\n",
            "# with split decoders, but a shared fully connected layer, this would have the effect of saving some\n",
            "# VRAM but possibly at the cost of introducing artefacts. For models with a shared decoder but split\n",
            "# fully connected layers, this would have the effect of increasing VRAM usage by processing some of\n",
            "# the upscales for each side rather than together.\n",
            "# \n",
            "# Выберите число между 0 и 6\n",
            "# [По умолчанию: 0]\n",
            "dec_upscales_in_fc = 0\n",
            "\n",
            "# Normalization to apply to apply after each upscale.\n",
            "#     - none - Do not apply a normalization layer\n",
            "#     - batch - Apply Batch Normalization\n",
            "#     - group - Apply Group Normalization\n",
            "#     - instance - Apply Instance Normalization\n",
            "#     - layer - Apply Layer Normalization (Ba et al., 2016)\n",
            "#     - rms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A simplified version\n",
            "# \t\tof Layer Normalization with reduced overhead.\n",
            "# \n",
            "# Выберите из: ['none', 'batch', 'group', 'instance', 'layer', 'rms']\n",
            "# [По умолчанию: none]\n",
            "dec_norm = none\n",
            "\n",
            "# The minimum number of filters to use in decoder upscalers (i.e. the number of filters to use for the\n",
            "# final upscale layer).\n",
            "# \n",
            "# Выберите число между 16 и 512\n",
            "# [По умолчанию: 64]\n",
            "dec_min_filters = 64\n",
            "\n",
            "# The maximum number of filters to use in decoder upscalers (i.e. the number of filters to use for the\n",
            "# first upscale layer).\n",
            "# \n",
            "# Выберите число между 256 и 5120\n",
            "# [По умолчанию: 512]\n",
            "dec_max_filters = 512\n",
            "\n",
            "# Alters the action of the filter slope.\n",
            "# \n",
            "#     - full: The number of filters at each upscale layer will reduce from the chosen max_filters at\n",
            "# \t\tthe first layer to the chosen min_filters at the last layer as dictated by the dec_filter_slope.\n",
            "#     - cap_max: The filters will decline at a fixed rate from each upscale to the next based on the\n",
            "# \t\tfilter_slope setting. If there are more upscales than filters, then the earliest upscales will be\n",
            "# \t\tcapped at the max_filter value until the filters can reduce to the min_filters value at the final\n",
            "# \t\tupscale. (EG: 512 -> 512 -> 512 -> 256 -> 128 -> 64).\n",
            "#     - cap_min: The filters will decline at a fixed rate from each upscale to the next based on the\n",
            "# \t\tfilter_slope setting. If there are more upscales than filters, then the earliest upscales will\n",
            "# \t\tdrop their filters until the min_filter value is met and repeat the min_filter value for the\n",
            "# \t\tremaining upscales. (EG: 512 -> 256 -> 128 -> 64 -> 64 -> 64).\n",
            "# \n",
            "# Выберите из: ['full', 'cap_max', 'cap_min']\n",
            "# [По умолчанию: full]\n",
            "dec_slope_mode = full\n",
            "\n",
            "# The rate that the filters reduce at each upscale layer.\n",
            "# \n",
            "#     - Full Slope Mode: Negative numbers will drop the number of filters quicker at first and slow\n",
            "# \t\tdown each upscale. Positive numbers will drop the number of filters slower at first but then speed\n",
            "# \t\tup each upscale. A value of 0.0 will reduce at a linear rate (i.e. the same number of filters will\n",
            "# \t\tbe reduced at each upscale).\n",
            "# \n",
            "#     - Cap Min/Max Slope Mode: Only positive values will work here. Negative values will\n",
            "# \t\tautomatically be converted to their positive counterpart. A value of 0.5 will halve the number of\n",
            "# \t\tfilters at each upscale until the minimum value is reached. A value of 0.33 will be reduce the\n",
            "# \t\tnumber of filters by a third until the minimum value is reached etc.\n",
            "# \n",
            "# Выберите десятичное число между -0.99 и 0.99\n",
            "# [По умолчанию: -0.45]\n",
            "dec_filter_slope = -0.45\n",
            "\n",
            "# The number of Residual Blocks to apply to each upscale layer. Set to 0 to disable residual blocks\n",
            "# entirely.\n",
            "# \n",
            "# Выберите число между 0 и 8\n",
            "# [По умолчанию: 1]\n",
            "dec_res_blocks = 1\n",
            "\n",
            "# The kernel size to apply to the final Convolution layer.\n",
            "# \n",
            "# Выберите число между 1 и 9\n",
            "# [По умолчанию: 5]\n",
            "dec_output_kernel = 5\n",
            "\n",
            "# Gaussian Noise acts as a regularization technique for preventing overfitting of data.\n",
            "#     - True - Apply a Gaussian Noise layer to each upscale.\n",
            "#     - False - Don't apply a Gaussian Noise layer to each upscale.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "dec_gaussian = True\n",
            "\n",
            "# If Residual blocks have been enabled, enabling this option will not apply a Residual block to the\n",
            "# final upscaler.\n",
            "#     - True - Don't apply a Residual block to the final upscale.\n",
            "#     - False - Apply a Residual block to all upscale layers.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: True]\n",
            "dec_skip_last_residual = True\n",
            "\n",
            "# If the command line option 'freeze-weights' is enabled, then the layers indicated here will be\n",
            "# frozen the next time the model starts up. NB: Not all architectures contain all of the layers listed\n",
            "# here, so any layers marked for freezing that are not within your chosen architecture will be\n",
            "# ignored. EG:\n",
            "#  If 'split fc' has been selected, then 'fc_a' and 'fc_b' are available for freezing. If it has not\n",
            "# been selected then 'fc_both' is available for freezing.\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Если выбираете несколько опций, тогда каждая опция должна быть разделена пробелом или запятой\n",
            "# (например: опция1, опция2, опция3)\n",
            "# \n",
            "# Выберите из: ['encoder', 'keras_encoder', 'fc_a', 'fc_b', 'fc_both', 'fc_shared', 'fc_gblock',\n",
            "# 'g_block_a', 'g_block_b', 'g_block_both', 'decoder_a', 'decoder_b', 'decoder_both']\n",
            "# [По умолчанию: keras_encoder]\n",
            "freeze_layers = keras_encoder\n",
            "\n",
            "# If the command line option 'load-weights' is populated, then the layers indicated here will be\n",
            "# loaded from the given weights file if starting a new model. NB Not all architectures contain all of\n",
            "# the layers listed here, so any layers marked for loading that are not within your chosen\n",
            "# architecture will be ignored. EG:\n",
            "#  If 'split fc' has been selected, then 'fc_a' and 'fc_b' are available for loading. If it has not\n",
            "# been selected then 'fc_both' is available for loading.\n",
            "# \n",
            "# Если выбираете несколько опций, тогда каждая опция должна быть разделена пробелом или запятой\n",
            "# (например: опция1, опция2, опция3)\n",
            "# \n",
            "# Выберите из: ['encoder', 'fc_a', 'fc_b', 'fc_both', 'fc_shared', 'fc_gblock', 'g_block_a',\n",
            "# 'g_block_b', 'g_block_both', 'decoder_a', 'decoder_b', 'decoder_both']\n",
            "# [По умолчанию: encoder]\n",
            "load_layers = encoder\n",
            "\n",
            "# Faceswap Encoder only: The number of convolutions to perform within the encoder.\n",
            "# \n",
            "# Выберите число между 2 и 10\n",
            "# [По умолчанию: 4]\n",
            "fs_original_depth = 4\n",
            "\n",
            "# Faceswap Encoder only: The minumum number of filters to use for encoder convolutions. (i.e. the\n",
            "# number of filters to use for the first encoder layer).\n",
            "# \n",
            "# Выберите число между 16 и 2048\n",
            "# [По умолчанию: 128]\n",
            "fs_original_min_filters = 128\n",
            "\n",
            "# Faceswap Encoder only: The maximum number of filters to use for encoder convolutions. (i.e. the\n",
            "# number of filters to use for the final encoder layer).\n",
            "# \n",
            "# Выберите число между 256 и 8192\n",
            "# [По умолчанию: 1024]\n",
            "fs_original_max_filters = 1024\n",
            "\n",
            "# Use a slightly alternate version of the Faceswap Encoder.\n",
            "#     - True - Use the alternate variation of the Faceswap Encoder.\n",
            "#     - False - Use the original Faceswap Encoder.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "fs_original_use_alt = False\n",
            "\n",
            "# The width multiplier for mobilenet encoders. Controls the width of the network. Values less than 1.0\n",
            "# proportionally decrease the number of filters within each layer. Values greater than 1.0\n",
            "# proportionally increase the number of filters within each layer. 1.0 is the default number of layers\n",
            "# used within the paper.\n",
            "# NB: This option is ignored for any non-mobilenet encoders.\n",
            "# NB: If loading ImageNet weights, then for MobilenetV1 only values of '0.25', '0.5', '0.75' or '1.0\n",
            "# can be selected. For MobilenetV2 only values of '0.35', '0.50', '0.75', '1.0', '1.3' or '1.4' can be\n",
            "# selected. For mobilenet_v3 only values of '0.75' or '1.0' can be selected\n",
            "# \n",
            "# Выберите десятичное число между 0.1 и 2.0\n",
            "# [По умолчанию: 1.0]\n",
            "mobilenet_width = 1.0\n",
            "\n",
            "# The depth multiplier for MobilenetV1 encoder. This is the depth multiplier for depthwise convolution\n",
            "# (known as the resolution multiplier within the original paper).\n",
            "# NB: This option is only used for MobilenetV1 and is ignored for all other encoders.\n",
            "# NB: If loading ImageNet weights, this must be set to 1.\n",
            "# \n",
            "# Выберите число между 1 и 10\n",
            "# [По умолчанию: 1]\n",
            "mobilenet_depth = 1\n",
            "\n",
            "# The dropout rate for MobilenetV1 encoder.\n",
            "# NB: This option is only used for MobilenetV1 and is ignored for all other encoders.\n",
            "# \n",
            "# Выберите десятичное число между 0.001 и 2.0\n",
            "# [По умолчанию: 0.001]\n",
            "mobilenet_dropout = 0.001\n",
            "\n",
            "# Use a minimilist version of MobilenetV3.\n",
            "# In addition to large and small models MobilenetV3 also contains so-called minimalistic models, these\n",
            "# models have the same per-layer dimensions characteristic as MobilenetV3 however, they don't utilize\n",
            "# any of the advanced blocks (squeeze-and-excite units, hard-swish, and 5x5 convolutions). While these\n",
            "# models are less efficient on CPU, they are much more performant on GPU/DSP.\n",
            "# NB: This option is only used for MobilenetV3 and is ignored for all other encoders.\n",
            "# \n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "mobilenet_minimalistic = False\n",
            "\n",
            "[model.realface]\n",
            "# AN EXTRA DETAILED VARIANT OF ORIGINAL MODEL.\n",
            "# INCORPORATES IDEAS FROM BRYANLYON AND INSPIRATION FROM THE VILLAIN MODEL.\n",
            "# REQUIRES ABOUT 6GB-8GB OF VRAM (BATCHSIZE 8-16).\n",
            "# \n",
            "\n",
            "# Resolution (in pixels) of the input image to train on.\n",
            "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
            "# Higher resolutions may increase prediction accuracy, but does not effect the resulting output size.\n",
            "# Must be between 64 and 128 and be divisible by 16.\n",
            "# \n",
            "# Выберите число между 64 и 128\n",
            "# [По умолчанию: 64]\n",
            "input_size = 64\n",
            "\n",
            "# Output image resolution (in pixels).\n",
            "# Be aware that larger resolution will increase VRAM requirements.\n",
            "# NB: Must be between 64 and 256 and be divisible by 16.\n",
            "# \n",
            "# Выберите число между 64 и 256\n",
            "# [По умолчанию: 128]\n",
            "output_size = 128\n",
            "\n",
            "# Number of nodes for decoder. Might affect your model's ability to learn in general.\n",
            "# Note that: Lower values will affect the ability to predict details.\n",
            "# \n",
            "# Выберите число между 768 и 2048\n",
            "# [По умолчанию: 1536]\n",
            "dense_nodes = 1536\n",
            "\n",
            "# Encoder Convolution Layer Complexity. sensible ranges: 128 to 150.\n",
            "# \n",
            "# Выберите число между 96 и 160\n",
            "# [По умолчанию: 128]\n",
            "complexity_encoder = 128\n",
            "\n",
            "# Decoder Complexity.\n",
            "# \n",
            "# Выберите число между 512 и 544\n",
            "# [По умолчанию: 512]\n",
            "complexity_decoder = 512\n",
            "\n",
            "[model.unbalanced]\n",
            "# AN UNBALANCED MODEL WITH ADJUSTABLE INPUT SIZE OPTIONS.\n",
            "# THIS IS AN UNBALANCED MODEL SO B>A SWAPS MAY NOT WORK WELL\n",
            "# \n",
            "\n",
            "# Resolution (in pixels) of the image to train on.\n",
            "# BE AWARE Larger resolution will dramatically increaseVRAM requirements.\n",
            "# Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).\n",
            "# NB: Your faceset must be at least 1.6x larger than your required input size.\n",
            "# (e.g. 160 is the maximum input size for a 256x256 faceset).\n",
            "# \n",
            "# Выберите число между 64 и 512\n",
            "# [По умолчанию: 128]\n",
            "input_size = 128\n",
            "\n",
            "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
            "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
            "# NB: lowmem will override cutom nodes and complexity settings.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "lowmem = False\n",
            "\n",
            "# Number of nodes for decoder. Don't change this unless you know what you are doing!\n",
            "# \n",
            "# Выберите число между 512 и 4096\n",
            "# [По умолчанию: 1024]\n",
            "nodes = 1024\n",
            "\n",
            "# Encoder Convolution Layer Complexity. sensible ranges: 128 to 160.\n",
            "# \n",
            "# Выберите число между 64 и 1024\n",
            "# [По умолчанию: 128]\n",
            "complexity_encoder = 128\n",
            "\n",
            "# Decoder A Complexity.\n",
            "# \n",
            "# Выберите число между 64 и 1024\n",
            "# [По умолчанию: 384]\n",
            "complexity_decoder_a = 384\n",
            "\n",
            "# Decoder B Complexity.\n",
            "# \n",
            "# Выберите число между 64 и 1024\n",
            "# [По умолчанию: 512]\n",
            "complexity_decoder_b = 512\n",
            "\n",
            "[model.villain]\n",
            "# A HIGHER RESOLUTION VERSION OF THE ORIGINAL MODEL BY VILLAINGUY.\n",
            "# EXTREMELY VRAM HEAVY. DON'T TRY TO RUN THIS IF YOU HAVE A SMALL GPU.\n",
            "# \n",
            "\n",
            "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
            "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
            "# \n",
            "# Выберите из: True, False\n",
            "# [По умолчанию: False]\n",
            "lowmem = False\n",
            "\n",
            "[trainer.original]\n",
            "# ORIGINAL TRAINER OPTIONS.\n",
            "# WARNING: THE DEFAULTS FOR AUGMENTATION WILL BE FINE FOR 99.9% OF USE CASES. ONLY CHANGE THEM IF YOU\n",
            "# ABSOLUTELY KNOW WHAT YOU ARE DOING!\n",
            "\n",
            "# Number of sample faces to display for each side in the preview when training.\n",
            "# \n",
            "# Выберите число между 2 и 16\n",
            "# [По умолчанию: 14]\n",
            "preview_images = 14\n",
            "\n",
            "# The opacity of the mask overlay in the training preview. Lower values are more transparent.\n",
            "# \n",
            "# Выберите число между 0 и 100\n",
            "# [По умолчанию: 30]\n",
            "mask_opacity = 30\n",
            "\n",
            "# The RGB hex color to use for the mask overlay in the training preview.\n",
            "# \n",
            "# [По умолчанию: #ff0000]\n",
            "mask_color = #ff0000\n",
            "\n",
            "# Percentage amount to randomly zoom each training image in and out.\n",
            "# \n",
            "# Выберите число между 0 и 25\n",
            "# [По умолчанию: 5]\n",
            "zoom_amount = 5\n",
            "\n",
            "# Percentage amount to randomly rotate each training image.\n",
            "# \n",
            "# Выберите число между 0 и 25\n",
            "# [По умолчанию: 10]\n",
            "rotation_range = 10\n",
            "\n",
            "# Percentage amount to randomly shift each training image horizontally and vertically.\n",
            "# \n",
            "# Выберите число между 0 и 25\n",
            "# [По умолчанию: 5]\n",
            "shift_range = 5\n",
            "\n",
            "# Percentage chance to randomly flip each training image horizontally.\n",
            "# NB: This is ignored if the 'no-flip' option is enabled\n",
            "# \n",
            "# Выберите число между 0 и 75\n",
            "# [По умолчанию: 50]\n",
            "flip_chance = 50\n",
            "\n",
            "# Percentage amount to randomly alter the lightness of each training image.\n",
            "# NB: This is ignored if the 'no-augment-color' option is enabled\n",
            "# \n",
            "# Выберите число между 0 и 75\n",
            "# [По умолчанию: 30]\n",
            "color_lightness = 30\n",
            "\n",
            "# Percentage amount to randomly alter the 'a' and 'b' colors of the L*a*b* color space of each\n",
            "# training image.\n",
            "# NB: This is ignored if the 'no-augment-color' optionis enabled\n",
            "# \n",
            "# Выберите число между 0 и 50\n",
            "# [По умолчанию: 8]\n",
            "color_ab = 8\n",
            "\n",
            "# Percentage chance to perform Contrast Limited Adaptive Histogram Equalization on each training\n",
            "# image.\n",
            "# NB: This is ignored if the 'no-augment-color' option is enabled\n",
            "# \n",
            "# Эта настройка будет обновлена для существующих моделей.\n",
            "# \n",
            "# Выберите число между 0 и 75\n",
            "# [По умолчанию: 50]\n",
            "color_clahe_chance = 50\n",
            "\n",
            "# The grid size dictates how much Contrast Limited Adaptive Histogram Equalization is performed on any\n",
            "# training image selected for clahe. Contrast will be applied randomly with a gridsize of 0 up to the\n",
            "# maximum. This value is a multiplier calculated from the training image size.\n",
            "# NB: This is ignored if the 'no-augment-color' option is enabled\n",
            "# \n",
            "# Выберите число между 1 и 8\n",
            "# [По умолчанию: 4]\n",
            "color_clahe_max_size = 4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDzky8jnfDzE",
        "outputId": "2d1e5adc-4953-45e0-da0a-df9dc8ad69ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Install Tensorflow\n",
        "\n",
        "!pip install -r /content/faceswap/requirements/requirements_nvidia.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring pywin32: markers 'sys_platform == \"win32\"' don't match your environment\n",
            "Ignoring pynvx: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Collecting tqdm>=4.65 (from -r /content/faceswap/requirements/_requirements_base.txt (line 1))\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting psutil>=5.9.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 2))\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting numexpr>=2.8.7 (from -r /content/faceswap/requirements/_requirements_base.txt (line 3))\n",
            "  Downloading numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 4))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting opencv-python<4.12.0.0,>=4.9.0.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 5))\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pillow<10.0.0,>=9.4.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 6))\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting scikit-learn>=1.3.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 7))\n",
            "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting fastcluster>=1.2.6 (from -r /content/faceswap/requirements/_requirements_base.txt (line 8))\n",
            "  Downloading fastcluster-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting matplotlib>=3.8.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 9))\n",
            "  Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting imageio>=2.33.1 (from -r /content/faceswap/requirements/_requirements_base.txt (line 10))\n",
            "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting imageio-ffmpeg<0.6.0,>=0.4.9 (from -r /content/faceswap/requirements/_requirements_base.txt (line 13))\n",
            "  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting ffmpy>=0.3.0 (from -r /content/faceswap/requirements/_requirements_base.txt (line 14))\n",
            "  Downloading ffmpy-0.6.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-ml-py<300,>=12.535 (from -r /content/faceswap/requirements/requirements_nvidia.txt (line 3))\n",
            "  Downloading nvidia_ml_py-13.580.65-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: tensorflow<2.11.0,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg<0.6.0,>=0.4.9->-r /content/faceswap/requirements/_requirements_base.txt (line 13)) (80.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (1.74.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.14.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.19.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (1.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.40.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.32.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.45.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (2025.8.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (0.6.1)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn>=1.3.0->-r /content/faceswap/requirements/_requirements_base.txt (line 7))\n",
            "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn>=1.3.0->-r /content/faceswap/requirements/_requirements_base.txt (line 7))\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.3.0->-r /content/faceswap/requirements/_requirements_base.txt (line 7))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: pip is looking at multiple versions of fastcluster to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastcluster>=1.2.6 (from -r /content/faceswap/requirements/_requirements_base.txt (line 8))\n",
            "  Downloading fastcluster-1.2.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.8.0->-r /content/faceswap/requirements/_requirements_base.txt (line 9))\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=3.8.0->-r /content/faceswap/requirements/_requirements_base.txt (line 9))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=3.8.0->-r /content/faceswap/requirements/_requirements_base.txt (line 9))\n",
            "  Downloading fonttools-4.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (108 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.8.0->-r /content/faceswap/requirements/_requirements_base.txt (line 9))\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.8.0->-r /content/faceswap/requirements/_requirements_base.txt (line 9)) (2.4.7)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib>=3.8.0->-r /content/faceswap/requirements/_requirements_base.txt (line 9))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11.0,>=2.10.0->-r /content/faceswap/requirements/requirements_nvidia.txt (line 5)) (3.0.2)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-13.580.65-py3-none-any.whl (48 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "Downloading numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (399 kB)\n",
            "Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastcluster-1.2.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Downloading ffmpy-0.6.1-py3-none-any.whl (5.5 kB)\n",
            "Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: nvidia-ml-py, tqdm, threadpoolctl, python-dateutil, psutil, pillow, numpy, kiwisolver, joblib, imageio-ffmpeg, fonttools, ffmpy, cycler, scipy, opencv-python, numexpr, imageio, fastcluster, contourpy, scikit-learn, matplotlib\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.2.6\n",
            "\u001b[2K    Uninstalling numpy-2.2.6:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fastcluster-1.2.6 ffmpy-0.6.1 fonttools-4.59.1 imageio-2.37.0 imageio-ffmpeg-0.5.1 joblib-1.5.1 kiwisolver-1.4.9 matplotlib-3.10.5 numexpr-2.11.0 numpy-1.26.4 nvidia-ml-py-13.580.65 opencv-python-4.11.0.86 pillow-9.5.0 psutil-7.0.0 python-dateutil-2.9.0.post0 scikit-learn-1.7.1 scipy-1.15.3 threadpoolctl-3.6.0 tqdm-4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "id": "-E2dI-bp-5tN",
        "outputId": "56e61794-8e39-4452-bb7a-c8217b97aa9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npA9qG2JnBSH",
        "outputId": "584acd0f-0431-460d-eb1d-432d1b78030c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 faceswap/tools.py mask -a 'face_b/alignments.fsa' -i 'face_b' -it faces -M vgg-clear -p all -o '/drive/MyDrive/colab_files/faceswap/faces/facexb/mask' -b 3 -t 4 -ot mask -L INFO\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First time configuration. Please select the required backend\n",
            "1: CPU, 2: DIRECTML, 3: NVIDIA, 4: APPLE SILICON, 5: ROCM: 3\n",
            "Faceswap config written to: /content/faceswap/config/.faceswap\n",
            "Setting Faceswap backend to NVIDIA\n",
            "usage: tools.py\n",
            "       [-h]\n",
            "       {alignments,effmpeg,manual,mask,model,preview,sort}\n",
            "       ...\n",
            "\n",
            "positional arguments:\n",
            "  {alignments,effmpeg,manual,mask,model,preview,sort}\n",
            "    alignments\n",
            "    This\n",
            "    command\n",
            "    lets you\n",
            "    perform\n",
            "    various\n",
            "    tasks\n",
            "    pertaining\n",
            "    to an\n",
            "    alignments\n",
            "    file.\n",
            "    effmpeg\n",
            "    This\n",
            "    command\n",
            "    allows you\n",
            "    to easily\n",
            "    execute\n",
            "    common\n",
            "    ffmpeg\n",
            "    tasks.\n",
            "    manual\n",
            "    This\n",
            "    command\n",
            "    lets you\n",
            "    perform\n",
            "    various\n",
            "    actions on\n",
            "    frames,\n",
            "    faces and\n",
            "    alignments\n",
            "    files using\n",
            "    visual\n",
            "    tools.\n",
            "    mask\n",
            "    This tool\n",
            "    allows you\n",
            "    to\n",
            "    generate,\n",
            "    import,\n",
            "    export or\n",
            "    preview\n",
            "    masks for\n",
            "    existing\n",
            "    alignments.\n",
            "    model\n",
            "    This tool\n",
            "    lets you\n",
            "    perform\n",
            "    actions on\n",
            "    saved\n",
            "    Faceswap\n",
            "    models.\n",
            "    preview\n",
            "    This\n",
            "    command\n",
            "    allows you\n",
            "    to preview\n",
            "    swaps to\n",
            "    tweak\n",
            "    convert\n",
            "    settings.\n",
            "    sort\n",
            "    This\n",
            "    command\n",
            "    lets you\n",
            "    sort images\n",
            "    using\n",
            "    various\n",
            "    methods.\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "tools.py: error: unrecognized arguments: mask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IvtNv6w3KGX"
      },
      "source": [
        "# Run Training\n",
        "Iteration number is up to you but it's recommended that you should do until the number you are okay with. You should check your timelapse folder for that.\n",
        "\n",
        "And if my calculations are right, trainer is doing nearly 360 iterations in 10 mins, which means 2160 iterations in 1 hr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAj-tLiJgUTM",
        "outputId": "94ab645a-ed23-4398-b6e8-f0702a121199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#set variables start\n",
        "num_iterations = \"100000\"\n",
        "save_every = \"360\"\n",
        "save_model_every = \"25000\"\n",
        "batch_num = \"16\"\n",
        "num_gpus = \"1\"\n",
        "\n",
        "trainer_type = \"villain\"\n",
        "\n",
        "model_dir = \"/content/drive/My Drive/colab_files/faceswap/models/YourModelName\"\n",
        "alignments_file_a = \"face_a/2_alignments.fsa\"\n",
        "alignments_file_b = \"face_b/alignments.fsa\"\n",
        "timelapse_dir = \"/content/drive/My Drive/colab_files/faceswap/output/timelapse\"\n",
        "#set variables end\n",
        "\n",
        "# фикс matplotlib backend (Colab без GUI)\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# запуск тренировки\n",
        "!python3 faceswap/faceswap.py train \\\n",
        "  -A 'face_a' \\\n",
        "  -B 'face_b'  \\\n",
        "  -m \"{model_dir}\" \\\n",
        "  -t \"{trainer_type}\" \\\n",
        "  -bs \"{batch_num}\" \\\n",
        "  -it \"{num_iterations}\" \\\n",
        "  -s \"{save_every}\" \\\n",
        "  -ss \"{save_model_every}\" \\\n",
        "  -tia 'face_a' \\\n",
        "  -tib 'face_b' \\\n",
        "  -to \"{timelapse_dir}\"\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting Faceswap backend to NVIDIA\n",
            "08/17/2025 12:52:18 INFO     Log level set to: INFO\n",
            "08/17/2025 12:52:20 ERROR    Got Exception on main handler:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/faceswap/lib/cli/launcher.py\", line 223, in execute_script\n",
            "    script = self._import_script()\n",
            "  File \"/content/faceswap/lib/cli/launcher.py\", line 53, in _import_script\n",
            "    module = import_module(mod)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/content/faceswap/scripts/train.py\", line 15, in <module>\n",
            "    from lib.gui.utils.image import TRAININGPREVIEW\n",
            "  File \"/content/faceswap/lib/gui/__init__.py\", line 4, in <module>\n",
            "    from lib.gui.command import CommandNotebook\n",
            "  File \"/content/faceswap/lib/gui/command.py\", line 9, in <module>\n",
            "    from .control_helper import ControlPanel\n",
            "  File \"/content/faceswap/lib/gui/control_helper.py\", line 15, in <module>\n",
            "    from .custom_widgets import ContextMenu, MultiOption, ToggledFrame, Tooltip\n",
            "  File \"/content/faceswap/lib/gui/custom_widgets.py\", line 14, in <module>\n",
            "    from .utils import get_config\n",
            "  File \"/content/faceswap/lib/gui/utils/__init__.py\", line 4, in <module>\n",
            "    from .config import get_config, initialize_config, PATHCACHE\n",
            "  File \"/content/faceswap/lib/gui/utils/config.py\", line 12, in <module>\n",
            "    from lib.gui._config import Config as UserConfig\n",
            "  File \"/content/faceswap/lib/gui/_config.py\", line 8, in <module>\n",
            "    from matplotlib import font_manager\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 1299, in <module>\n",
            "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\", line 774, in __setitem__\n",
            "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
            "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
            "08/17/2025 12:52:20 CRITICAL An unexpected crash has occurred. Crash report written to '/content/faceswap/crash_report.2025.08.17.125220485701.log'. You MUST provide this file if seeking assistance. Please verify you are running the latest version of faceswap before reporting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n"
      ],
      "metadata": {
        "id": "vqJ88UCWFmnv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow tensorflow-gpu keras keras-preprocessing"
      ],
      "metadata": {
        "id": "uzJIWIZ0B83_",
        "outputId": "1b9748fe-8727-4a5e-fe01-7e758cf9baf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: keras 3.10.0\n",
            "Uninstalling keras-3.10.0:\n",
            "  Successfully uninstalled keras-3.10.0\n",
            "\u001b[33mWARNING: Skipping keras-preprocessing as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.10.1"
      ],
      "metadata": {
        "id": "mwy8KoQKCEex",
        "outputId": "f4a87105-eb02-4687-f4b3-177ad86ac045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.10.1\n",
            "  Downloading tensorflow-2.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.10.1)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.10.1)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=2.0 (from tensorflow==2.10.1)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10.1)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.10.1)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.10.1)\n",
            "  Downloading grpcio-1.74.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.10.1)\n",
            "  Downloading h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.1)\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.1)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.10.1)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting numpy>=1.20 (from tensorflow==2.10.1)\n",
            "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.10.1)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow==2.10.1)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.1)\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.10.1) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.10.1) (1.16.0)\n",
            "Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.1)\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.10.1)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.1)\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow==2.10.1)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow==2.10.1)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow==2.10.1)\n",
            "  Downloading wrapt-1.17.3-cp39-cp39-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.3.6)\n",
            "Collecting requests<3,>=2.21.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.45.1)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading charset_normalizer-3.4.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/lib/python3/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (4.6.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.2.0)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1)\n",
            "  Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading tensorflow-2.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m  \u001b[33m0:00:16\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading grpcio-1.74.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m159.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m154.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m196.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Downloading wrapt-1.17.3-cp39-cp39-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (81 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Installing collected packages: tensorboard-plugin-wit, libclang, keras, flatbuffers, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, packaging, opt-einsum, numpy, MarkupSafe, idna, grpcio, google-pasta, gast, charset_normalizer, certifi, cachetools, astunparse, absl-py, werkzeug, rsa, requests, pyasn1-modules, keras-preprocessing, h5py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "\u001b[2K  Attempting uninstall: MarkupSafe\n",
            "\u001b[2K    Found existing installation: MarkupSafe 2.0.1\n",
            "\u001b[2K    Uninstalling MarkupSafe-2.0.1:\n",
            "\u001b[2K      Successfully uninstalled MarkupSafe-2.0.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/37\u001b[0m [tensorflow]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.40.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 idna-3.10 keras-2.10.0 keras-preprocessing-1.1.2 libclang-18.1.1 numpy-2.0.2 opt-einsum-3.4.0 packaging-25.0 protobuf-3.19.6 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.4 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.14.1 urllib3-2.5.0 werkzeug-3.1.3 wrapt-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "!python get-pip.py"
      ],
      "metadata": {
        "id": "Mxc1-nnbDdLT",
        "outputId": "357c2ba5-a272-496f-efec-bc4affbe5969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-17 12:42:52--  https://bootstrap.pypa.io/get-pip.py\n",
            "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
            "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2148718 (2.0M) [text/x-python]\n",
            "Saving to: ‘get-pip.py.1’\n",
            "\n",
            "\rget-pip.py.1          0%[                    ]       0  --.-KB/s               \rget-pip.py.1        100%[===================>]   2.05M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-08-17 12:42:53 (49.7 MB/s) - ‘get-pip.py.1’ saved [2148718/2148718]\n",
            "\n",
            "Collecting pip\n",
            "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.2 setuptools-80.9.0 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "ffNz16JHCyFY",
        "outputId": "14411356-d344-4fc0-8970-c7aa759c5e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Переключаемся на Python 3.10 + ставим pip + TensorFlow 2.10 ---\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.10 python3.10-dev python3.10-distutils\n",
        "\n",
        "# Ставим Python 3.10 как основной\n",
        "!sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.10 2\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2\n",
        "!sudo update-alternatives --set python /usr/bin/python3.10\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "\n",
        "# Ставим свежий pip под 3.10\n",
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10\n",
        "\n",
        "# Проверка версии Python и pip\n",
        "!python --version\n",
        "!pip --version\n",
        "\n",
        "# Ставим TensorFlow 2.10.0 (GPU версия)\n",
        "!pip install tensorflow==2.10.0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yh3XLuryC7Jb",
        "outputId": "221ca506-d969-4d52-ce55-f467f00476c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,937 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,271 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,521 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,209 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Fetched 12.0 MB in 3s (4,750 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.10).\n",
            "python3.10 set to manually installed.\n",
            "python3.10-dev is already the newest version (3.10.12-1~22.04.10).\n",
            "python3.10-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "update-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python (python) in auto mode\n",
            "update-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.2 setuptools-80.9.0 wheel-0.45.1\n",
            "Python 3.10.12\n",
            "pip 25.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting tensorflow==2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow==2.10.0)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow==2.10.0)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=2.0 (from tensorflow==2.10.0)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow==2.10.0)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.10.0)\n",
            "  Downloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.10.0)\n",
            "  Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.0)\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.10.0)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting numpy>=1.20 (from tensorflow==2.10.0)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow==2.10.0)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow==2.10.0)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.10.0) (1.16.0)\n",
            "Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.0)\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.10.0)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.0)\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow==2.10.0)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow==2.10.0)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow==2.10.0)\n",
            "  Downloading wrapt-1.17.3-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.3.6)\n",
            "Collecting requests<3,>=2.21.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.45.1)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.0)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m159.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m208.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m199.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Downloading wrapt-1.17.3-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (81 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Installing collected packages: tensorboard-plugin-wit, libclang, keras, flatbuffers, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, packaging, opt-einsum, numpy, MarkupSafe, idna, grpcio, google-pasta, gast, charset_normalizer, certifi, cachetools, astunparse, absl-py, werkzeug, rsa, requests, pyasn1-modules, keras-preprocessing, h5py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "\u001b[2K  Attempting uninstall: MarkupSafe\n",
            "\u001b[2K    Found existing installation: MarkupSafe 2.0.1\n",
            "\u001b[2K    Uninstalling MarkupSafe-2.0.1:\n",
            "\u001b[2K      Successfully uninstalled MarkupSafe-2.0.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/37\u001b[0m [tensorflow]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.40.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 idna-3.10 keras-2.10.0 keras-preprocessing-1.1.2 libclang-18.1.1 numpy-2.2.6 opt-einsum-3.4.0 packaging-25.0 protobuf-3.19.6 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.4 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.14.1 urllib3-2.5.0 werkzeug-3.1.3 wrapt-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zeZ9ShYFw3K"
      },
      "source": [
        "# Convert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkHZf2BN86go"
      },
      "source": [
        "!python3 faceswap/faceswap.py convert -i '/content/drive/My Drive/colab_files/faceswap/Editor.avi' -o '/content/drive/My Drive/colab_files/faceswap/Results' -al '/content/drive/My Drive/colab_files/faceswap/2_alignments.fsa' -m '/content/drive/My Drive/colab_files/faceswap/models/YourModelName' -c match-hist -M none -w ffmpeg -osc 100 -l 0.4 -j 0 -t realface -L INFO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}